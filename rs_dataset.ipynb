{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rs_dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrbFYrZrRfQL3chwKh94i9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitriyValetov/nlp_course_project/blob/master/rs_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFKkGYW1zr6D",
        "colab_type": "text"
      },
      "source": [
        "RossiyaSegodnya Dataset for PyTorch\n",
        "[github repository](https://github.com/RossiyaSegodnya/ria_news_dataset)\n",
        "\n",
        "Full dataset 1003869 news \n",
        "\n",
        "https://github.com/RossiyaSegodnya/ria_news_dataset/raw/master/ria.json.gz\n",
        "\n",
        "20 news \n",
        "\n",
        "https://raw.githubusercontent.com/RossiyaSegodnya/ria_news_dataset/master/ria_20.json\n",
        "\n",
        "1000 news\n",
        "\n",
        "https://raw.githubusercontent.com/RossiyaSegodnya/ria_news_dataset/master/ria_1k.json\n",
        "\n",
        "normalized divided into sentences dataset (use second block)\n",
        "\n",
        "https://drive.google.com/uc?id=1-UtATnzLE809Vi6RLgy3GRHX2TXRzhd6&export=download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q3WZAoQ0uPm",
        "colab_type": "text"
      },
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Q6RHX-1cMY",
        "colab_type": "text"
      },
      "source": [
        "## Raw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFknxhyhzcs8",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "dc7d71ab-0656-4afe-9579-203478e45224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/RossiyaSegodnya/ria_news_dataset/master/ria_20.json\"\n",
        "# url = \"https://raw.githubusercontent.com/RossiyaSegodnya/ria_news_dataset/master/ria_1k.json\"\n",
        "# url = \"https://github.com/RossiyaSegodnya/ria_news_dataset/raw/master/ria.json.gz\"\n",
        "fn, ext = os.path.splitext(os.path.basename(url))\n",
        "print(f'downloading {fn + ext}')\n",
        "r = requests.get(url) \n",
        "with open(fn + ext, 'wb') as f:\n",
        "  f.write(r.content)\n",
        "if ext == '.gz':  # requests should decompress .gz by default but don't...\n",
        "  print(f'decompressing from {fn + ext} to {fn}')\n",
        "  import gzip\n",
        "  import shutil\n",
        "  with gzip.open(fn + ext, 'rb') as gz_file:\n",
        "    with open(fn, 'wb') as json_file:\n",
        "      shutil.copyfileobj(gz_file, json_file)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading ria_20.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2iHIFxB1eKF",
        "colab_type": "text"
      },
      "source": [
        "## Normalized and Tokenized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dah85FHSuEJs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "859fe491-b057-41ce-805c-4c75b06f2fc7"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "file_id = '1-UtATnzLE809Vi6RLgy3GRHX2TXRzhd6'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "fn = downloaded.metadata['title']\n",
        "print(f'downloading: {fn}')\n",
        "downloaded.GetContentFile(fn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading: norm_sents_ria.json.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD4HsWfv0l8n",
        "colab_type": "text"
      },
      "source": [
        "# Normalize and Tokenize\n",
        "Skip this if you loaded a normalized dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xApnxmfu_erC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # for sentence tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtbAmYvi4HQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Files are not clear json files. They contain json strings line by line.\n",
        "fn = 'ria.json'\n",
        "norm_fn = f'norm_sents_{fn}'  # news divided into normalized sentences\n",
        "with open(fn) as f:\n",
        "  with open(norm_fn, 'a+') as nf:\n",
        "    for line in tqdm(f, total=1003869):  # 1003869 - news at full dataset\n",
        "      n = json.loads(line)\n",
        "      text = BeautifulSoup(n['text']).get_text()\n",
        "      title = BeautifulSoup(n['title']).get_text()\n",
        "      text_ss = sent_tokenize(text)  # text sentences\n",
        "      title_ss = sent_tokenize(title)  # title sentences\n",
        "      norm_text_ss = [' '.join(w for w in word_tokenize(s) if w.isalnum()) for s in text_ss]\n",
        "      norm_title_ss = [' '.join(w for w in word_tokenize(s) if w.isalnum()) for s in title_ss]\n",
        "      json_str = json.dumps({'text': norm_text_ss, 'title': norm_title_ss}, ensure_ascii=False)\n",
        "      nf.write(json_str + '\\n')\n",
        "with open(norm_fn) as nf: \n",
        "  print(json.loads(next(nf)))  # print first news"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgQFgkGcIiVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "comp_fn = norm_fn + '.gz'\n",
        "# compress normalized dataset\n",
        "print(f'compressing {norm_fn} to {comp_fn}')\n",
        "with gzip.open(comp_fn + '.gz', 'wb') as gz_file:\n",
        "  with open(norm_fn, 'rb') as json_file:\n",
        "    shutil.copyfileobj(json_file, gz_file)\n",
        "\n",
        "# check\n",
        "with gzip.open(comp_fn, 'rb') as gz_file:\n",
        "  print(json.loads(next(gz_file)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO83gr3P1Cqc",
        "colab_type": "text"
      },
      "source": [
        "# Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfmnj01Aw_y8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d87aaeb9-75aa-4c74-9bde-468d5dd8b1e7"
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "comp_fn = 'norm_sents_ria.json.gz'\n",
        "\n",
        "# check\n",
        "with gzip.open(comp_fn, 'rb') as gz_file:\n",
        "  print(json.loads(next(gz_file)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['москва 31 янв риа новости', 'большая часть из 33 детей которых граждане сша пытались вывезти из гаити в организованный в доминиканской республике приют не являются сиротами сообщает в воскресенье агентство франс пресс со ссылкой на заявление представителя международной организации детские деревни sos sos children village оказывающей помощь детям оставшимся без родителей как заявила агентству патрисия варгас patricia vargas курирующая программы детских деревень sos в центральной америке мексике и на карибах поговорив с детьми она выяснила что родители многих из них живы', 'некоторые дети смогли назвать свои домашние адреса и номера телефонов что дает возможность связаться с их родителями', 'в это воскресенье гаитянская полиция задержала десятерых граждан сша подозреваемых в попытке без разрешения вывезти более 30 детей в доминиканскую республику', 'представитель баптистской церкви в городе меридиан американского штата айдахо шон лэнкфорд sean lankford заявил что задержанные прибыли на гаити в составе группы помогающей детям которые остались без родителей после разрушительного землетрясения 12 января', 'лэнкфорд также сообщил что в числе задержанных его дочь и жена и они думали что у них имеются все необходимые документы позволяющие вывезти детей в организованный в доминиканской республике приют', 'в настоящее время все эти дети за исключением маленькой девочки страдающей от истощения которая была госпитализирована находятся в благотворительном центре организации в городе croix des bouquets расположенном в 12 километрах к от столицы гаити', 'по словам варгас точный возраст малышки не известен врачи полагают что ей около 7 месяцев', 'центр детских деревень sos на гаити юридически не является сиротским приютом и не отдает детей на усыновление', 'ранее гаитянские сообщали что за детьми оставшимися сиротами после землетрясения охотятся педофилы и торговцы людьми как отмечает франс пресс после разгула стихии на гаити были установлены новые правила усыновления согласно которым страны бельрив должен лично разрешить вывоз сирот', 'целью подобных мер является пресечение попыток незаконного вывоза детей для преступных целей в условиях хаоса царящего в стране после разгула стихии', 'по данным ответственных лиц на гаити тысячи детей могли быть разлучены с родителями или лишиться их в результате двух землетрясений магнитудой 7 и произошедших у побережья этого островного государства 12 января'], 'title': ['большинство детей которых пытались увезти в сша из гаити не сироты']}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}