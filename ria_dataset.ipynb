{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ria_dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiku3qbZGBAADgOS2TELlz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitriyValetov/nlp_course_project/blob/master/ria_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFKkGYW1zr6D",
        "colab_type": "text"
      },
      "source": [
        "RossiyaSegodnya dataset\n",
        "[github repository](https://github.com/RossiyaSegodnya/ria_news_dataset)\n",
        "\n",
        "## Raw\n",
        "\n",
        "Full dataset 1003869 news \n",
        "\n",
        "https://github.com/RossiyaSegodnya/ria_news_dataset/raw/master/ria.json.gz\n",
        "\n",
        "20 news \n",
        "\n",
        "https://raw.githubusercontent.com/RossiyaSegodnya/ria_news_dataset/master/ria_20.json\n",
        "\n",
        "1000 news\n",
        "\n",
        "https://raw.githubusercontent.com/RossiyaSegodnya/ria_news_dataset/master/ria_1k.json\n",
        "\n",
        "## Processed\n",
        "*   html parsing (BeautifulSoup)\n",
        "*   split into sentences (nltk)\n",
        "*   split into words (nltk)\n",
        "*   filter words (python str.isalnum())\n",
        "\n",
        "https://drive.google.com/open?id=1-UtATnzLE809Vi6RLgy3GRHX2TXRzhd6\n",
        "\n",
        "*   html parsing (BeautifulSoup)\n",
        "*   split into sentences (nltk)\n",
        "*   split into words (nltk)\n",
        "*   filter words (python str.isalnum())\n",
        "*   stopwords (nltk)\n",
        "\n",
        "https://drive.google.com/open?id=1bhsdkXYEe4qixPddK9DkaQ-7z0jAn5Bi\n",
        "\n",
        "*   html parsing (BeautifulSoup)\n",
        "*   split into sentences (nltk)\n",
        "*   split into words (nltk)\n",
        "*   filter words (python str.isalnum())\n",
        "*   stopwords (nltk)\n",
        "*   lemmatization (pymorphy2)\n",
        "\n",
        "https://drive.google.com/open?id=1-40QXRckYZIfJTPiAhtQ8LAeBKks8ITx\n",
        "\n",
        "*   html parsing (BeautifulSoup)\n",
        "*   split into sentences (nltk)\n",
        "*   split into words (nltk)\n",
        "*   filter words (python str.isalnum())\n",
        "*   stopwords (nltk)\n",
        "*   stemming (nltk snowball)\n",
        "\n",
        "https://drive.google.com/open?id=1m7unmZmh0B3DJ-hiLm8-a5WoXECMc4cV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q3WZAoQ0uPm",
        "colab_type": "text"
      },
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Q6RHX-1cMY",
        "colab_type": "text"
      },
      "source": [
        "## Raw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFknxhyhzcs8",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "# url = \"https://raw.githubusercontent.com/RossiyaSegodnya/ria_news_dataset/master/ria_20.json\"\n",
        "url = \"https://raw.githubusercontent.com/RossiyaSegodnya/ria_news_dataset/master/ria_1k.json\"\n",
        "# url = \"https://github.com/RossiyaSegodnya/ria_news_dataset/raw/master/ria.json.gz\"\n",
        "fn, ext = os.path.splitext(os.path.basename(url))\n",
        "print(f'downloading {fn + ext}')\n",
        "r = requests.get(url) \n",
        "with open(fn + ext, 'wb') as f:\n",
        "  f.write(r.content)\n",
        "# if ext == '.gz':  # requests should decompress .gz by default but don't...\n",
        "#   print(f'decompressing from {fn + ext} to {fn}')\n",
        "#   import gzip\n",
        "#   import shutil\n",
        "#   with gzip.open(fn + ext, 'rb') as gz_file:\n",
        "#     with open(fn, 'wb') as json_file:\n",
        "#       shutil.copyfileobj(gz_file, json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2iHIFxB1eKF",
        "colab_type": "text"
      },
      "source": [
        "## Processed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dah85FHSuEJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YouKrfikcvfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "# 1-UtATnzLE809Vi6RLgy3GRHX2TXRzhd6  # norm\n",
        "# 1bhsdkXYEe4qixPddK9DkaQ-7z0jAn5Bi  # stop\n",
        "# 1-40QXRckYZIfJTPiAhtQ8LAeBKks8ITx  # lem\n",
        "# 1m7unmZmh0B3DJ-hiLm8-a5WoXECMc4cV  # snow\n",
        "file_id = '1-UtATnzLE809Vi6RLgy3GRHX2TXRzhd6'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "fn = downloaded.metadata['title']\n",
        "print(f'downloading: {fn}')\n",
        "downloaded.GetContentFile(fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD4HsWfv0l8n",
        "colab_type": "text"
      },
      "source": [
        "# Process (skip if dataset is processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xApnxmfu_erC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # for sentence tokenization\n",
        "nltk.download('stopwords')\n",
        "!pip install pymorphy2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtbAmYvi4HQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.notebook import tqdm\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import gzip\n",
        "import os\n",
        "\n",
        "morph = MorphAnalyzer()  # lemmatizer\n",
        "snow = SnowballStemmer('russian')  # Porter stemmer doesn't work with russian\n",
        "stop = stopwords.words('russian')\n",
        "# Files are not clear json files. They contain json strings line by line.\n",
        "fn = 'ria_1k.json'  # raw path\n",
        "nfn = 'norm_ria_1k.json'  # processed path\n",
        "f = gzip.open(fn, 'rb') if os.path.splitext(fn)[1] == '.gz' else open(fn)\n",
        "cnt = 0\n",
        "if os.path.exists(nfn):\n",
        "  with open(nfn) as nf:\n",
        "    for line in nf:\n",
        "      cnt += 1\n",
        "  with open(nfn) as nf:\n",
        "    for _ in range(cnt - 1):\n",
        "      next(nf)\n",
        "    print(f'{cnt} sample: {json.loads(next(nf))}')  # check consistency\n",
        "  with gzip.open(fn, 'rb') if os.path.splitext(fn)[1] == '.gz' else open(fn) as f:\n",
        "    for _ in range(cnt - 1):\n",
        "      next(f)\n",
        "    print(f'{cnt} sample: {json.loads(next(f))}')  # check consistency\n",
        "print(f'start from {cnt + 1} sample')\n",
        "with gzip.open(fn, 'rb') if os.path.splitext(fn)[1] == '.gz' else open(fn) as f:\n",
        "  for _ in range(cnt):  # skip already processed samples\n",
        "    next(f)\n",
        "  with open(nfn, 'a+') as nf:\n",
        "    for line in tqdm(f, initial=cnt, total=1003869):  # 1003869 (full dataset)\n",
        "      n = json.loads(line)\n",
        "      # PARSE HTML\n",
        "      text = BeautifulSoup(n['text']).get_text()\n",
        "      title = BeautifulSoup(n['title']).get_text()\n",
        "      # NORM\n",
        "      # norm_text_ss = [' '.join(w for w in word_tokenize(s) \n",
        "      # if w.isalnum()) for s in sent_tokenize(text)]\n",
        "      # norm_title_ss = [' '.join(w for w in word_tokenize(s)\n",
        "      #  if w.isalnum()) for s in sent_tokenize(title)]\n",
        "      # NORM STOP\n",
        "      # norm_text_ss = [' '.join(w for w in word_tokenize(s) \n",
        "      # if w.isalnum() and w not in stop) for s in sent_tokenize(text)]\n",
        "      # norm_title_ss = [' '.join(w for w in word_tokenize(s)\n",
        "      #  if w.isalnum() and w not in stop) for s in sent_tokenize(title)]\n",
        "      # NORM + STOP + LEM\n",
        "      # norm_text_ss = [' '.join(morph.parse(w)[0].normal_form for w in word_tokenize(s) \n",
        "      # if w.isalnum() and w not in stop) for s in sent_tokenize(text)]\n",
        "      # norm_title_ss = [' '.join(morph.parse(w)[0].normal_form for w in word_tokenize(s) \n",
        "      # if w.isalnum() and w not in stop) for s in sent_tokenize(title)]\n",
        "      # STOP + STOP + STEM\n",
        "      norm_text_ss = [' '.join(snow.stem(w) for w in word_tokenize(s) \n",
        "      if w.isalnum() and w not in stop) for s in sent_tokenize(text)]\n",
        "      norm_title_ss = [' '.join(snow.stem(w) for w in word_tokenize(s) \n",
        "      if w.isalnum() and w not in stop) for s in sent_tokenize(title)]\n",
        "      json_str = json.dumps({'text': norm_text_ss, 'title': norm_title_ss}, ensure_ascii=False)\n",
        "      nf.write(json_str + '\\n')\n",
        "with open(nfn, 'rb') as f:  # first check\n",
        "  print(json.loads(next(f)))\n",
        "with open(nfn, 'rb') as f:  # all check\n",
        "  for line in tqdm(f):\n",
        "    json.loads(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgQFgkGcIiVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "cfn = nfn + '.gz'\n",
        "# compress normalized dataset\n",
        "print(f'compressing {nfn} to {cfn}')\n",
        "with gzip.open(cfn, 'wb') as gz_file:\n",
        "  with open(nfn, 'rb') as json_file:\n",
        "    shutil.copyfileobj(json_file, gz_file)\n",
        "\n",
        "with gzip.open(cfn, 'rb') as f:  # first check\n",
        "  print(json.loads(next(f)))\n",
        "with gzip.open(cfn, 'rb') as f:  # all check\n",
        "  for line in tqdm(f):\n",
        "    json.loads(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn-bDtnAc66r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uploaded = drive.CreateFile({'title': cfn})\n",
        "# uploaded.SetContentFile(cfn)\n",
        "# uploaded.Upload()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from shutil import copyfile\n",
        "copyfile(cfn, '/content/drive/My Drive/' + cfn)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}