{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_transformer_headline_generation_simple.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB8HXRc_chEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'num_epochs': 20,\n",
        "    'num_tokens': 2000,\n",
        "    'vocab_size': 10000+4,\n",
        "    'd_model': 512,\n",
        "    'num_encoder_layers': 6,\n",
        "    'num_decoder_layers': 6,\n",
        "    'dim_feedforward': 2048,\n",
        "    'nhead': 8,\n",
        "    'pos_dropout': 0.1,\n",
        "    'trans_dropout': 0.1,\n",
        "    'n_warmup_steps': 4000,\n",
        "    'batch_size': 512,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRpxX6rCdGVM",
        "colab_type": "code",
        "outputId": "4c1f167e-249f-41fc-e0dc-75faf4ee53ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root = \"drive/My Drive/Colab Notebooks/huawei/project/generations/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KEJUKEFM8OR",
        "colab_type": "code",
        "outputId": "733abfcf-609d-4eda-ae62-487ac6ee3164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!pip install pymorphy2\n",
        "!pip install einops"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.8)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.393442.3710985)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.6/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okcgOCw7M-tL",
        "colab_type": "code",
        "outputId": "657dd07a-8e25-4983-84c6-5098b0a91bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "import pymorphy2\n",
        "import re\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "\n",
        "def save_texts(texts, titles, filename):\n",
        "  dump_texts = os.path.join(root, filename)\n",
        "  with open(file=dump_texts, mode='w') as f:\n",
        "    json.dump({'texts': texts, 'titles': titles}, f)\n",
        "\n",
        "def load_texts(filename):\n",
        "  dump_texts = os.path.join(root, filename)\n",
        "  with open(file=dump_texts, mode='r') as f:\n",
        "    data = json.load(f)\n",
        "  return data['texts'], data['titles']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Using cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qnoS4q_NCbT",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess ria news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WgfjFFtNDmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source_file = \"processed-ria.json\"\n",
        "# titles = []\n",
        "# texts = []\n",
        "\n",
        "# bad_text = []\n",
        "\n",
        "# with open(file=os.path.join(root, source_file), mode='r') as f:\n",
        "#   # lines = f.readlines()[:100]\n",
        "#   for _ in tqdm.tqdm(range(100000)):\n",
        "#     line = f.readline()\n",
        "#     try:\n",
        "#       line_j = json.loads(line)\n",
        "#       text = nltk.sent_tokenize(BeautifulSoup(line_j['text'], \"lxml\").text)[1]\n",
        "#       titles.append(line_j['title'])\n",
        "#       texts.append(text)\n",
        "#     except IndexError:\n",
        "#       bad_text.append(line_j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrVZU4kvNF9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# morph = pymorphy2.MorphAnalyzer()\n",
        "# pattern = re.compile(\"^[а-яА-Я]+$\")\n",
        "\n",
        "# texts_processed = []\n",
        "# titles_processed = []\n",
        "\n",
        "# def process_sentence(sentence):\n",
        "#     words = [w for w in nltk.word_tokenize(sentence) if w not in stopwords.words(\"russian\") and pattern.match(w)]\n",
        "#     words = [morph.parse(word)[0].normal_form for word in words]\n",
        "#     return words\n",
        "\n",
        "# for text_i in tqdm.tqdm(range(len(texts))):\n",
        "#   try:\n",
        "#     # sentences = nltk.sent_tokenize(texts[text_i])\n",
        "#     # texts_processed.append(process_sentence(sentences[0]))\n",
        "#     texts_processed.append(process_sentence(texts[text_i]))\n",
        "#     titles_processed.append(process_sentence(titles[text_i]))\n",
        "#   except IndexError as e:\n",
        "#     print(e)\n",
        "#     print(text_i)\n",
        "\n",
        "# texts = texts_processed\n",
        "# titles = titles_processed\n",
        "# save_texts(texts, titles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmpGQn5VNL_p",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-uC9447NOjo",
        "colab_type": "code",
        "outputId": "a66000ff-ad7d-4df1-a176-7a27de75e299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "source": [
        "texts, titles = load_texts(\"ria_text_titles.json\")\n",
        "texts, titles = texts[:10000], titles[:10000]\n",
        "print(max([len(t) for t in texts]))\n",
        "\n",
        "\n",
        "\n",
        "SOS_token = 0 # start of string.\n",
        "EOS_token = 1 # end of string.\n",
        "PAD_token = 2\n",
        "UNK_token = 3\n",
        "\n",
        "class Lang: # like a vocabulary\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": PAD_token, 'UNK': UNK_token}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", PAD_token: \"PAD\", UNK_token: \"UNK\"}\n",
        "        self.n_words = len(self.word2index)\n",
        "        self.max_seq_length = None\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        if self.max_seq_length is None:\n",
        "          self.max_seq_length = len(sentence)\n",
        "        elif self.max_seq_length < len(sentence):\n",
        "          self.max_seq_length = len(sentence)\n",
        "        for word in sentence:\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def tensor2text(self, tensor):\n",
        "        return [self.index2word[token.item()] for token in tensor]\n",
        "        \n",
        "    def ids2text(self, numbers):\n",
        "      return [self.index2word[num] for num in numbers]\n",
        "\n",
        "    def text2ids(self, tokens):\n",
        "        return [self.word2index[token] if token in self.word2index else self.word2index['UNK'] for token in tokens]\n",
        "            \n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "\n",
        "def prepareData(texts, titles):\n",
        "    pairs = [[txt, ttl] for txt, ttl in zip(texts, titles) if len(txt)>0 and len(ttl)>0 ]\n",
        "    lang = Lang('ru')\n",
        "\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    for pair in pairs:\n",
        "        lang.addSentence(pair[0])\n",
        "        lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(lang.name, lang.n_words)\n",
        "    return lang, pairs\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(lang, pair[0])\n",
        "    target_tensor_t = tensorFromSentence(lang, pair[1][:-1])\n",
        "    target_tensor_l = tensorFromSentence(lang, pair[1])\n",
        "    return (input_tensor, target_tensor_t, target_tensor_l)\n",
        "\n",
        "\n",
        "lang, pairs = prepareData(texts, titles)\n",
        "print(random.choice(pairs))\n",
        "\n",
        "lens = [len(t) for t in texts]\n",
        "sns.distplot(lens)\n",
        "\n",
        "i = 100\n",
        "print(texts[i])\n",
        "print(titles[i])\n",
        "print(lang.max_seq_length)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "134\n",
            "Read 9981 sentence pairs\n",
            "Counted words:\n",
            "ru 19113\n",
            "[['проститься', 'известный', 'саксофонист', 'дирижёр', 'композитор', 'концертный', 'зал', 'имя', 'чайковский', 'проходить', 'гражданский', 'панихида', 'прислать', 'десятка', 'почитатель', 'талант'], ['прощание', 'георгий', 'гаранян']]\n",
            "['роджер', 'федерер', 'четыре', 'свой', 'карьера', 'выиграть', 'открытый', 'чемпионат', 'австралия', 'теннис']\n",
            "['роджер', 'федерер', 'четыре', 'выиграть', 'открытый', 'чемпионат', 'австралия']\n",
            "134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3Sc913n8fd3ZqSRdZclWb5HvqWJk7RJ6iQESoGGQsKWGpaUJmWXwOZslgNZLoXDprCb7eZwdgm721BolpLTlraBkHZTCt4SyJamhy5tamzHbhrHtS07jixfdZd1v8x3/5hnnMl4ZI2kmXlmRp/XOTqeeZ7fzHz12Pro59/ze36PuTsiIlK5ImEXICIihaWgFxGpcAp6EZEKp6AXEalwCnoRkQoXC7uATG1tbd7Z2Rl2GSIiZeXAgQN97t6ebV/JBX1nZyf79+8PuwwRkbJiZm/Mt09DNyIiFU5BLyJS4RT0IiIVTkEvIlLhFPQiIhVOQS8iUuEU9CIiFU5BLyJS4RT0IiIVruSujF3JntnbnXX7h+7YXORKRKSSqEcvIlLhFPQiIhVOQS8iUuEU9CIiFU5BLyJS4RT0IiIVTkEvIlLhFPQiIhVOQS8iUuEU9CIiFU5BLyJS4RT0IiIVTkEvIlLhFPQiIhVOQS8iUuFyCnozu9vMjppZl5k9kmV/3My+EOzfa2adafvebmYvmdlhM/uumdXkr3wREVnIgkFvZlHgSeAeYCdwv5ntzGj2IDDo7tuBJ4DHg9fGgD8HfsndbwB+GJjJW/UiIrKgXHr0twNd7n7S3aeBZ4HdGW12A58LHj8H3GVmBvwY8Iq7fwfA3fvdfS4/pYuISC5yCfoNwOm05z3Btqxt3H0WGAZagWsBN7MXzOxlM/vtbB9gZg+Z2X4z29/b27vY70FERK6i0CdjY8C7gJ8L/vxpM7srs5G7P+Xuu9x9V3t7e4FLKn1//+o5vnywh7NDE2GXIiIVIJebg58BNqU93xhsy9amJxiXbwL6Sfb+v+HufQBm9jxwK/C1ZdZdsaZm5vh/x/twYN+pQXaua+T+2zeRHAkTEVm8XHr0+4AdZrbFzKqB+4A9GW32AA8Ej+8FXnR3B14AbjKz2uAXwA8Br+Wn9MrUPTiOA/fdtonbOlt47dwIF0amwi5LRMrYgkEfjLk/TDK0jwBfdPfDZvaYmb0/aPZpoNXMuoAPA48Erx0EPkbyl8Uh4GV3/9v8fxuV41TfOAZc29HATRuaATjZNxpuUSJS1nIZusHdnweez9j2aNrjSeAD87z2z0lOsZQcvNE/xrrmGmqqorQ3xAF4vW+M79/WFnJlIlKudGVsCZlNJDg9OM41rXUANNTEqIoar/eOhVyZiJQzBX0JOTc0ycyc0xkEfcSMtvo4J/sU9CKydAr6EnKqPxnona21l7e11sd5XUEvIsugoC8hp/rHaa2rpqGm6vK2tvpqugfGmZlLhFiZiJQzBX2JSCScN/rHLo/Pp7TXx5lLON0D4yFVJiLlTkFfIk72jTI+PfeWYRuAtvpg5o1OyIrIEinoS8SBNwYB2JwR9K311QAapxeRJctpHr0U3sHuIVZVRS/34FNqq2PUVkf5v69doC7+1r+uD92xuZglikiZUo++RLzcPcim1auIZFnTpr0+Tt+olkEQkaVR0JeAkckZjl8cZdPq2qz72xT0IrIMCvoS8MrpYdxhc8t8QV/NpclZpmZ0zxYRWTwFfQk42J08EbtxnqBvDcbt+8ami1aTiFQOBX0JOHh6iO1r6llVHc26vy1Y3KzvkoZvRGTxFPQhc3cOdg9y6+bmedu01lVjoHF6EVkSBX3ITvWPMzg+wy2bW+ZtUxWN0FxbpaAXkSVR0IcsNT5/y1V69JCaeaMxehFZPAV9yA52D1Efj7FjTcNV27UGUyyTd2gUEcmdgj5kB08P8o5NTUQjV7/5d3t9NVOzCUanZotUmYhUCgV9iCam5zhy7hK3bJp/fD4ltTSChm9EZLEU9CF6pWeIuYQvOD4P6UGvE7IisjgK+hAdPD0EwM2bFg76ptoqYhHTXHoRWTQFfYgOdg9yTWvt5StfryZixuq6avXoRWTRcgp6M7vbzI6aWZeZPZJlf9zMvhDs32tmncH2TjObMLNDwdcn81t++XJ3Xu4e4tarzJ/P1N6gKZYisngLrkdvZlHgSeC9QA+wz8z2uPtrac0eBAbdfbuZ3Qc8Dnww2HfC3W/Oc91l7+zwJL2XpnIan09pq4/zvXOXmEv4grN0RERScunR3w50uftJd58GngV2Z7TZDXwuePwccJdZloXV5bLLF0rlMOMmpa2+mjl3hsbVqxeR3OUS9BuA02nPe4JtWdu4+ywwDLQG+7aY2UEz+0cz+8FsH2BmD5nZfjPb39vbu6hvoFy9/MYQ8ViE69Zd/UKpdJp5IyJLUeiTseeAze5+C/Bh4Bkza8xs5O5Pufsud9/V3t5e4JJKw8HTg7x9YxNV0dz/CjSXXkSWIpeUOQNsSnu+MdiWtY2ZxYAmoN/dp9y9H8DdDwAngGuXW3S5m5qd4/CZkUWdiAWorY6yqiqqHr2ILEouQb8P2GFmW8ysGrgP2JPRZg/wQPD4XuBFd3czaw9O5mJmW4EdwMn8lF6+jl8YZXouwU0bmxb1OjOjrb6aXgW9iCzCgrNu3H3WzB4GXgCiwGfc/bCZPQbsd/c9wKeBp82sCxgg+csA4N3AY2Y2AySAX3L3gUJ8I6Xqmb3dV2zbdyp5CG5cv7igB2hvqOHYhUvLrktEVo4Fgx7A3Z8Hns/Y9mja40ngA1le9yXgS8usseKcHZqgIR5j8zw3A7+atU01vNw9yKXJmQJUJiKVSFfGhuDs0ATXr28ksoS58OuaagC4MKLhGxHJjYK+yBLunB+ZXNKwDUBHYzLozw9P5LMsEalgCvoi67s0xcycc8P6K2aZ5qQ+HqOhJsa54ck8VyYilUpBX2Rng574jRuW1qMHWNtYw/kRBb2I5EZBX2RnhyaJRYxt7XVLfo91TTVcvDTFzFwij5WJSKVS0BfZ2aEJ1jbVEFvEFbGZ1jbVMJdwXu8by2NlIlKpFPRF5O6cHZ5gfdOqZb3P2sbk64+cG8lHWSJS4RT0RTQ0PsPkTIJ1zTXLep/2hjhRM46c04VTIrIwBX0RnRlKnojd0Ly8Hn00YqxpjPO98+rRi8jCFPRFdGFkEuPNufDLsbaxhu+pRy8iOVDQF1Hf6BTNtVWLWpp4PmubklMsB8e0ZLGIXJ2Cvoj6x6ZprVv4RuC5WBv8r+CIhm9EZAEK+iJxd/pGp2itr87L+6WGf45fGM3L+4lI5VLQF8n49ByTMwla6/PTo2+oidFYE9OSxSKyIAV9kfQHNwtpq8tPj97MeNvaBvXoRWRBCvoi6QtOmuarRw+wo6OBoxcu4e55e08RqTwK+iLpH53CgJa6qry957Vr6hmemKH3ktamF5H5KeiLpG90mpa6amKR/B3ya9c2AHBU4/QichUK+iLpH5uiNU/j8ynXdiSD/pjG6UXkKhT0ReDu9I9O53V8HqCtPk5rXTXHzqtHLyLzU9AXwejULFOzCdryNIc+3Y6Oeo5dVNCLyPwU9EXQPxrMuMnTVbHpru1ITrHUzBsRmU9OQW9md5vZUTPrMrNHsuyPm9kXgv17zawzY/9mMxs1s9/KT9nlpX8smENfgB79tR0NjE7Nclb3kBWReSwY9GYWBZ4E7gF2Aveb2c6MZg8Cg+6+HXgCeDxj/8eAv1t+ueWpb3SaiEFzbWGCHtAVsiIyr1x69LcDXe5+0t2ngWeB3RltdgOfCx4/B9xlZgZgZj8FvA4czk/J5ad/dIqW2mqiEcv7e1/bUQ+gE7IiMq9cgn4DcDrteU+wLWsbd58FhoFWM6sH/gPwX5ZfavnqH5umLc8zblKaa6tZ0xDXFEsRmVehT8Z+FHjC3a+aQmb2kJntN7P9vb29BS6p+AbGkhdLFcr2NfWc6FXQi0h2uQT9GWBT2vONwbasbcwsBjQB/cAdwB+Y2Sng14HfMbOHMz/A3Z9y913uvqu9vX3R30Qpm5ieY2o2QUtt/pY+yLStPRn0mnkjItnEcmizD9hhZltIBvp9wIcy2uwBHgBeAu4FXvRk6vxgqoGZfRQYdfdP5KHusjE4npxamXki9pm93Xn7jG3tdVyanKV3dIo1Dcu/TaGIVJYFe/TBmPvDwAvAEeCL7n7YzB4zs/cHzT5Ncky+C/gwcMUUzJVqaHwGoKA9+u1rkjNvui5q+EZErpRLjx53fx54PmPbo2mPJ4EPLPAeH11CfWVvvh59Pm1bUwfAid4xvn9bW8E+R0TKk66MLbCh8WmqokZddbRgn7G2sYa66ign1KMXkSwU9AU2OD5Dc201wWUFBWFmbNPMGxGZh4K+wIYmpgs6Pp+yrb1ePXoRyUpBX2CDYzMFHZ9P2dZex9nhScamZgv+WSJSXhT0BTQ6NcvEzBwtRQj67WuSSyGc7B0r+GeJSHlR0BfQmcEJAJqLNHQDaJxeRK6goC+gM0PjAEXp0V/TWkc0Ygp6EbmCgr6AeorYo6+ORbhmda0umhKRKyjoC+jM4ASxiFEfz+m6tGXb2q4pliJyJQV9AfUMTtC0qopIAefQp9u+pp7X+8aYmUsU5fNEpDwo6AuoZ2iioMsTZ7p+XQMzc85xrU0vImkU9AV0ZnCc5lWFH59PuWlDEwCvnh0u2meKSOlT0BfI5MwcfaOFveFIps7WOurjMV49o6AXkTcp6Avk8oybIvboIxFj5/pGBb2IvIWCvkDODCWDvhhz6NPduL6J186NMKsTsiISKM68vxWoZzB5sVQx5tCnu3FDI5MzCf74xS46Gt96t6kP3bG5qLWISGlQj75AUnPoG4s4dANvnpA9G/yPQkREQV8gPYMTrGuuKdoc+pSt7fVURe3y0JGIiIK+QM4MTbCxubbonxuNGOuaVqlHLyKXKegLpGdwnA0tq0L57PXNqzg7PEnCPZTPF5HSoqAvgKnZOS5emmJjSEG/obmG6dkE/aPToXy+iJQWBX0BnBuaxB02NIfXowc0Ti8igIK+IFIBu7Gl+GP0AGsaaoiacX54MpTPF5HSktM8ejO7G/g4EAU+5e6/n7E/DnweeCfQD3zQ3U+Z2e3AU6lmwEfd/cv5Kr5UpebQb2xZxet9hbu13zN7u7Nuj0aMNY1xzo+oRy8iOfTozSwKPAncA+wE7jeznRnNHgQG3X078ATweLD9VWCXu98M3A38qZlV/EVaZwYniBisbapZuHGBrG2sUY9eRIDchm5uB7rc/aS7TwPPArsz2uwGPhc8fg64y8zM3cfdfTbYXgOsiGkgPYMTrG2soSoa3sjY2qYaRiZnGZuaXbixiFS0XJJoA3A67XlPsC1rmyDYh4FWADO7w8wOA98Ffikt+C8zs4fMbL+Z7e/t7V38d1FieoYmQhufT0n9b+L8iHr1IitdwYdR3H0vcIOZXQ98zsz+zt0nM9o8RTCWv2vXrrLs9aePlx87f4nOtrp5x9CLYW2wzs354Um2tdeHVoeIhC+XHv0ZYFPa843BtqxtgjH4JpInZS9z9yPAKHDjUostB3MJZ2RyhpYiL2aWqaGmivp4TOP0IpJT0O8DdpjZFjOrBu4D9mS02QM8EDy+F3jR3T14TQzAzK4BrgNO5aXyEjUyMUPCobnIyxNns7apRkM3IrLw0I27z5rZw8ALJKdXfsbdD5vZY8B+d98DfBp42sy6gAGSvwwA3gU8YmYzQAL4ZXfvK8Q3UioGJ5JXoxZ7Hfps1jbW8O2T/cwlnGikuIuriUjpyGmM3t2fB57P2PZo2uNJ4ANZXvc08PQyaywrQ2MzQPHXoc9mbVMNswmnf2yKNQ3hTfUUkXDpytg8S/Xoi3kLwfmsa3rzhKyIrFwK+jwbGpuhoSZGLMQ59Cnt9XEipqAXWenCT6MKMzA+XRLj8wCxaIT2hjjnFPQiK5qCPs8Gx6dZXVcaQQ/Q0VjDxUsKepGVTEGfR7OJBMPjMyUV9O0NcYbGZ5iZS4RdioiEREGfR8PjMzilMbUypb0+jgN9o1NhlyIiIVHQ59HAWHLGTan16AF6LynoRVYqBX0eDYyXXtC31ccxFPQiK5mCPo8Gx6aJRoyGmtJZcr8qGqGlrppeDd2IrFgK+jwaGJumpbaKiJXWcgPt9XH16EVWMAV9Hg2U2NTKlPaGOH2jUyQSZbkCtIgsk4I+jwbHZkpqxk1Ke32cmTnn7LDuISuyEino82Rieo6JmbmS7NG3BTNvui6OhlyJiIRBQZ8nqRk3JdmjD4L+RO9YyJWISBgU9HlSinPoU+rjMWqro5zoVY9eZCVS0OfJYAkHPSTH6U9o6EZkRVLQ58nA+DSrqqLUVEXDLiWr9oa4hm5EVigFfZ4MjpXm1MqU1BTL4fGZsEsRkSJT0OfJQIkH/ZrghOyR8yMhVyIixaagz4O5hDNUYssTZ9rYUgvAwe6hkCsRkWJT0OfB6YFx5txpLeGgr4vH2NJWx8vdg2GXIiJFpqDPg2MXLgGwprEm5Equ7pbNzRzsHsRdSyGIrCQ5Bb2Z3W1mR82sy8weybI/bmZfCPbvNbPOYPt7zeyAmX03+PM9+S2/NBwPpi2mxsFL1a2bW+gbneb0gJZCEFlJFgx6M4sCTwL3ADuB+81sZ0azB4FBd98OPAE8HmzvA37S3W8CHgCezlfhpeTYhUs0raoq2amVKbdsbgbQ8I3ICpNLj/52oMvdT7r7NPAssDujzW7gc8Hj54C7zMzc/aC7nw22HwZWmVlpd3uX4NiFUToaS//beltHA7XVUQW9yAqTS9BvAE6nPe8JtmVt4+6zwDDQmtHmZ4CX3f2KhdHN7CEz229m+3t7e3OtvSTMJZwTvaN0NJT2+DxALBrhHRubFfQiK0xRTsaa2Q0kh3P+Xbb97v6Uu+9y913t7e3FKClv3ugfY3o2UfInYlNuvaaZI+cuMT49G3YpIlIkuQT9GWBT2vONwbasbcwsBjQB/cHzjcCXgZ939xPLLbjUHLuQPBFbDkM3z+zt5tLELHMJ54mvHueZvd1hlyQiRZBL0O8DdpjZFjOrBu4D9mS02UPyZCvAvcCL7u5m1gz8LfCIu38zX0WXkuPB1Mr2Ep9xk7JpdfLCqe6B8ZArEZFiWTDogzH3h4EXgCPAF939sJk9ZmbvD5p9Gmg1sy7gw0BqCubDwHbgUTM7FHytyft3EaJjF0fZ2LKKeKy0Z9yk1MVjtNXHOdWnBc5EVopYLo3c/Xng+Yxtj6Y9ngQ+kOV1vwf83jJrLGnHL1zi2o6GsMtYlK1tdXynZ4g53UNWZEXQlbHLMDuX4GTvGDs66sMuZVG2ttcxNZvg7JAunBJZCRT0y3Cqf5zpuQTXrimvHv2WtjoATuqOUyIrgoJ+GVInYstt6Kahpoo1DXFOapxeZEVQ0C/Da+dGiBhsX1NeQzeQHL45FVwDICKVTUG/DIdOD3Hd2kZWVZfHjJt0W9vqmZlzXunR+vQilU5Bv0SJhHOoe+jyQmHlJjVO/9KJ/pArEZFCU9Av0cm+US5NzXLzpvIM+rp4jLWNNbx0UkEvUukU9Ev0cnBLvls2t4RcydJtba/jwBuDTM3OhV2KiBSQgn6JDp0eorEmxtZgCKQcbW2rZ2o2ofvIilQ4Bf0SHewe4h2bmolELOxSlmxLWx1mGqcXqXQK+iUYm5rl6PmRsh62AVhVHeWG9Y0apxepcAr6JfjumWESDreU6YnYdHdubeVQ9xCTMxqnF6lUCvolSI1pl+uMm3R3bmtlei7BgTd01ymRSqWgX4JDpwfZ0lZHS1112KUs222dq4lGTOP0IhVMQb9I7s7L3UMVMWwDyXVvbtzQpHF6kQqmoF+kN/rH6b00xa7O1WGXkjd3bm3lO6eHGJvSfWRFKpGCfpH2nRoA4LbO8p5xk+7Oba3MJpz9GqcXqUgK+kXaf2qQ5toqtrWX34qV89l1TQsxjdOLVCwF/SLtOzXArmtayvpCqUx18Rjv2NSscXqRCqWgX4S+0SlO9o1xWwWNz6fcubWVV88Mc2lyJuxSRCTPcro5uCTtP5Ucwx4cm+aZvd0hV5Nfd25r5RNf72LfqQHec11H2OWISB6pR78I+08NEIsY65tXhV1K3r3zmhaqoxGN04tUoJyC3szuNrOjZtZlZo9k2R83sy8E+/eaWWewvdXMvm5mo2b2ifyWXnz7Tg2wsaWWWLTyfj/WVEW5ebPG6UUq0YKJZWZR4EngHmAncL+Z7cxo9iAw6O7bgSeAx4Ptk8B/An4rbxWHZHx6llfPjtDZWht2KQVz59ZWDp8dYXhc4/QilSSXruntQJe7n3T3aeBZYHdGm93A54LHzwF3mZm5+5i7/xPJwC9re08OMJdwOst4/fmF3LmtFXfY+7p69SKVJJeg3wCcTnveE2zL2sbdZ4FhoDXXIszsITPbb2b7e3t7c31ZUf3NoTNlf6ORhdyyuZl4LKLhG5EKUxKDze7+lLvvcvdd7e3tYZdzhbGpWV44fIF/8fb1FTk+nxKPRXnnNS06IStSYXKZXnkG2JT2fGOwLVubHjOLAU1AWadF+vTJQ6cHmZiZoz5eebNRM6eJ1lbH+NaJfgbGplldAatzikhuPfp9wA4z22Jm1cB9wJ6MNnuAB4LH9wIvurvnr8xwHTo9RPOqKq6p4BOxKdvak0NTezV8I1IxFgz6YMz9YeAF4AjwRXc/bGaPmdn7g2afBlrNrAv4MHB5CqaZnQI+BvyCmfVkmbFT0kanZum6OJq8P6xVzrIH89nQsoqqqGmcXqSC5DQW4e7PA89nbHs07fEk8IF5Xtu5jPpC90rPEAmvjLtJ5SIWidDZWqdxepEKUrlnFvNgLuG8dKKfDc2r6GisCbucotnaVsfxi6P0XpoKuxQRyQMF/VW80jNE/9g0P/K20psJVEhbgyWYv63hG5GKoKCfx1zCefF7F1nXVMP16xrDLqeo1jevoj4e0zi9SIVQ0M/jzd78GmwFnIRNF40Yt3W28G2N04tUBAV9FnMJ5+tHL7K2sYad61dWbz7lzm2tnOwb4+zQRNiliMgyKeiz+D/fOUvf6DTvuW7NiphSmc2P7VwLwHMHekKuRESWS0GfYS7h/NGLx+lojK/Y3jxAZ1sdP7ijjWf2djM7lwi7HBFZBgV9hq+8cpaTvWO857qOFdubh+TSCNesruP8yCT/ec9hntnbXXF31RJZKRT0aeYSzse/dpy3dTRwwwruzae8bW0DTauq2HtyIOxSRGQZFPRpvnSgh5O9Y/zqXTtWdG8+JTn7ZjVdvaP06eIpkbKloCc5TPFH/3Cc//jXr9LZWsvg+HTYJZWM2zpbiBh880Rf2KWIyBIp6IGZuQR/ua+bWNT44G2b1ZtP01BTxa7O1ew7NaBevUiZWvFBPz2b4G8OneHc8CT3vnMjTauqwi6p5Nx13Rpi0QgvvHY+7FJEZAlWdNCf7B3lZ/7kW7zcPcR7rlvDdWt1Ajabhpoq3r2jjcNnRzjwhk7MipSbFRv0B94Y4H1//E+cHhznX92xmR+9viPskkrau7a301AT4/f+9giJRMXcU0ZkRViRQX96YJyHPn+AjsYa/v7X3s3O9U1hl1TyqmMRfnznWg52D/GZb74edjkisgiVdxPUq3hmbzeTM3N88h9PMDY9y8/f2cmL37sYdlll45bNzQxNTPMHLxzlh65tZ0dHQ9gliUgOVlyP/q8PnaFvdIoP3X4N7Q3xsMspK2bGf/uXb6c+HuM3vniIGS2NIFIWVlTQHz1/iVd6hvmR69awfU192OWUpfaGOP/1p2/i1TMj/NvP72dkcibskkRkASsm6MenZ9nznTO018f5oR0r645R+fTM3m4GxqbZffN6vnGsl7v+5z/yxFePhV2WiFzFign6j3/tOIPjM/zULRuIRVfMt10wd2xp5Rd/YAujk7N8/GvHufsPv8ETXz1Gz+B42KWJSAZzL62pcrt27fL9+/fn9T2/1dXHv/7MP3PzpmZ+5taNeX3vlW5kYoZXeoboG5tm/6nkHPu7ru/goXdv5bbO1SFXJ7JymNkBd9+VbV/Fz7o5PTDOLz/zMlvb6njfTevCLqfiNK6q4l3BUNgPX9vO3tcH+GZXH1997QJb2+p49Cd3cue2VuKxaMiViqxcOfXozexu4ONAFPiUu/9+xv448HngnUA/8EF3PxXs+wjwIDAH/Kq7v3C1z8pnj35kcoaf/eRLnB2aYM/D7+JbugdqUUzPJth3aoBvHOvl0tQsNVURbutczda2OjqaaljXVENHY/KrrS5O46rYirsvr0i+LatHb2ZR4EngvUAPsM/M9rj7a2nNHgQG3X27md0HPA580Mx2AvcBNwDrgX8ws2vdfW5531J27s7EzBz9o9M8d6CHP/vm64xOzfLZX7ydzrY6BX2RVMci/MD2Nm7fspqui6N0XRzl+IVR9p0aYHLmyimZVVGjpbaa1vo4rXXVtNZX01oXZ11TDeubV9FSW0W8KkJ1NEp1LEJ1LELUDDOCLyNiUB2NEK+KEo9FiEVswV8e7s5cwpmZc2YSCWbnnNm5BDMJpypixGPR4HMjRCL2lte5Q8KdueAxJJd1zuVzF5J6f089nqddJPi+V/IvycyOarZ+a+ambJ3b+Y7xm//OFneM0/8OE/N0pov595fL0M3tQJe7nwQws2eB3UB60O8GPho8fg74hCWr3w086+5TwOtm1hW830v5Kf9Nh04P8bN/+hLTs28GyXt3dvDv37Odt29szvfHSQ6qohGuX9fI9eveXENoejbByMQMw5MzjEzMMDY9x9jU7OWvN/rHeO3cCKNTs2/5u1ysiCU/HzJ+iD31RzLgc/9e7HK4L7QChFkyICIRIxr8EDtv/uDjb32eCvOlni4zS4ZGKpSikSuDI/O9s/36uLJNFnl6nysCOmubbAWEIxIc40jELj9O/XtI/R0m/M0/F/vesUiESAR+4qZ1fOxnb857/bkE/QbgdNrzHuCO+dq4+6yZDQOtwfZvZ7x2Q+YHmNlDwEPB01EzO5pT9Qv4VPCVpg0ol4XVVWvhlFO9qrUwSrLWo8ATH8y6K5d6r5lvR0mcjHX3p4CnCv05ZrZ/vjGsUs6U8KcAAAUASURBVKNaC6ec6lWthVFOtcLy681lQvkZYFPa843BtqxtzCwGNJE8KZvLa0VEpIByCfp9wA4z22Jm1SRPru7JaLMHeCB4fC/woicH4fYA95lZ3My2ADuAf85P6SIikosFh26CMfeHgRdITq/8jLsfNrPHgP3uvgf4NPB0cLJ1gOQvA4J2XyR54nYW+JVCzbjJUcGHh/JItRZOOdWrWgujnGqFZdZbclfGiohIfmnRFxGRCqegFxGpcCsi6M3sbjM7amZdZvZI2PVkMrNNZvZ1M3vNzA6b2a8F21eb2VfN7HjwZ0vYtaaYWdTMDprZV4LnW8xsb3CMvxCcuA+dmTWb2XNm9j0zO2Jmd5bqcTWz3wj+/l81s780s5pSOq5m9hkzu2hmr6Zty3osLemPgrpfMbNbS6DW/x78O3jFzL5sZs1p+z4S1HrUzH487FrT9v2mmbmZtQXPl3RcKz7o05ZwuAfYCdwfLM1QSmaB33T3ncD3Ab8S1PgI8DV33wF8LXheKn4NOJL2/HHgCXffDgySXBajFHwc+Ht3vw54B8maS+64mtkG4FeBXe5+I8mJD6nlRErluH4WuDtj23zH8h6Ss+x2kLwY8k+KVGPKZ7my1q8CN7r724FjwEcAMpZquRv4X0FuFMtnubJWzGwT8GNAd9rmpR3X5JoMlfsF3Am8kPb8I8BHwq5rgZr/huTaQkeBdcG2dcDRsGsLatlI8of6PcBXACN51V4s2zEPsc4m4HWCSQdp20vuuPLm1eWrSc6G+wrw46V2XIFO4NWFjiXwp8D92dqFVWvGvp8G/iJ4/JZMIDnD8M6wayW5nMw7gFNA23KOa8X36Mm+hMMVyzCUCjPrBG4B9gId7n4u2HUe6AiprEx/CPw2kFqMphUYcvfZ4HmpHOMtQC/wZ8Ew06fMrI4SPK7ufgb4HyR7b+eAYeAApXlc0813LEv95+7fAH8XPC65Ws1sN3DG3b+TsWtJta6EoC8bZlYPfAn4dXcfSd/nyV/foc+FNbP3ARfd/UDYteQgBtwK/Im73wKMkTFMU0LHtYXkIoBbSK70WkeW/86XslI5lgsxs98lOVz6F2HXko2Z1QK/Azyar/dcCUFfFsswmFkVyZD/C3f/q2DzBTNbF+xfB1wMq740PwC838xOAc+SHL75ONAcLH8BpXOMe4Aed98bPH+OZPCX4nH9UeB1d+919xngr0ge61I8runmO5Yl+XNnZr8AvA/4ueAXE5RerdtI/sL/TvBzthF42czWssRaV0LQ57KEQ6jMzEheXXzE3T+Wtit9aYkHSI7dh8rdP+LuG929k+SxfNHdfw74OsnlL6B0aj0PnDaztwWb7iJ5lXbJHVeSQzbfZ2a1wb+HVK0ld1wzzHcs9wA/H8wS+T5gOG2IJxSWvIHSbwPvd/f0mxuX1FIt7v5dd1/j7p3Bz1kPcGvw73lpx7WYJxzC+gJ+guRZ9hPA74ZdT5b63kXyv7yvAIeCr58gOfb9NeA48A/A6rBrzaj7h4GvBI+3kvzh6AL+NxAPu76grpuB/cGx/WugpVSPK/BfgO8BrwJPA/FSOq7AX5I8fzAThM+D8x1Lkifonwx+5r5LcjZR2LV2kRzfTv2MfTKt/e8GtR4F7gm71oz9p3jzZOySjquWQBARqXArYehGRGRFU9CLiFQ4Bb2ISIVT0IuIVDgFvYhIhVPQi4hUOAW9iEiF+/8wBJWpTfp7dQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0FOQBL0NU0Z",
        "colab_type": "text"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br5sXOantT0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class HeadlinesDataset(Dataset):\n",
        "    def __init__(self, pairs, lang, batch_size):\n",
        "        self.pairs = pairs\n",
        "        self.lang = lang\n",
        "        self.batch_size = batch_size\n",
        "        self.batch_num = len(self.pairs)//batch_size\n",
        "        \n",
        "    def __getitem__(self, index): # todo: make number of pads depend on current batch max text, title length\n",
        "        srcs = []\n",
        "        srcs_lns = []\n",
        "        tgts = []\n",
        "        tgts_lns = []\n",
        "        for i in range(self.batch_size):\n",
        "          src, tgt = self.pairs[index*self.batch_size+i]\n",
        "          srcs.append(src)\n",
        "          srcs_lns.append(len(src))\n",
        "          tgts.append(tgt)\n",
        "          tgts_lns.append(len(tgt))\n",
        "\n",
        "        max_src_ln = max(srcs_lns)          \n",
        "        max_tgt_ln = max(tgts_lns)    \n",
        "\n",
        "        converted_srcs = []    \n",
        "        converted_tgts = []    \n",
        "        srcs_padding_masks = []\n",
        "        tgts_padding_masks = []\n",
        "\n",
        "        for i in range(len(srcs)):\n",
        "          converted_srcs.append(self.lang.text2ids(srcs[i]) + [self.lang.word2index['PAD']]*(max_src_ln-len(srcs[i]))) \n",
        "          srcs_padding_masks.append([False]*len(srcs[i]) + [True]*(max_src_ln-len(srcs[i])))\n",
        "\n",
        "          converted_tgts.append(\n",
        "              [self.lang.word2index['SOS']] + \n",
        "                self.lang.text2ids(tgts[i]) + \n",
        "              [self.lang.word2index['EOS']] + \n",
        "              [self.lang.word2index['PAD']]*(max_tgt_ln-len(tgts[i]))\n",
        "              ) \n",
        "          tgts_padding_masks.append([False]*(len(tgts[i])+2) + [True]*(max_tgt_ln-len(tgts[i])))\n",
        "\n",
        "        return (np.array(converted_srcs), np.array(converted_tgts), np.array(srcs_padding_masks), np.array(tgts_padding_masks))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.batch_num\n",
        "\n",
        "\n",
        "# train_dataset = HeadlinesDataset(pairs[1024:], lang, batch_size=config['batch_size'])\n",
        "# test_dataset  = HeadlinesDataset(pairs[:1024], lang, batch_size=config['batch_size'])\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "# src, tgt, src_key_padding_mask, tgt_key_padding_mask =  next(iter(train_loader))\n",
        "# src, tgt, src_key_padding_mask, tgt_key_padding_mask =  next(iter(test_loader))\n",
        "# print(src)\n",
        "# print(tgt)\n",
        "# print(src_key_padding_mask)\n",
        "# print(tgt_key_padding_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTf3m8gdNcJr",
        "colab_type": "text"
      },
      "source": [
        "# Prepear word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wty7UgXlNIXZ",
        "colab_type": "code",
        "outputId": "4ac20957-431c-401e-9ed7-4f7b04fa3ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "from random import shuffle\n",
        "\n",
        "union = []\n",
        "for pair in tqdm.tqdm(pairs):\n",
        "  union.append(['SOS']+pair[0]+['EOS']+['PAD']+['UNK'])\n",
        "\n",
        "for pair in tqdm.tqdm(pairs):\n",
        "  union.append(['SOS']+pair[1]+['EOS']+['PAD']+['UNK'])\n",
        "\n",
        "shuffle(union)\n",
        "\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "model = Word2Vec(union, \n",
        "                 min_count = 0, \n",
        "                 workers=cpu_count(),\n",
        "                 size=config['d_model'],\n",
        "                 window=10,\n",
        "                 negative=5\n",
        "                )\n",
        "\n",
        "\n",
        "init_voc = set(lang.word2index.keys())\n",
        "wv_voc = set(model.wv.vocab.keys())\n",
        "print(len(wv_voc), len(init_voc))\n",
        "print(wv_voc - init_voc)\n",
        "print(init_voc - wv_voc)\n",
        "# assert(len(init_voc)==len(wv_voc))\n",
        "\n",
        "\n",
        "word_init_vectors = {}\n",
        "for w in model.wv.vocab:\n",
        "    word_init_vectors[w] = model[w].tolist()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9981/9981 [00:00<00:00, 511758.10it/s]\n",
            "100%|██████████| 9981/9981 [00:00<00:00, 822753.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "19113 19113\n",
            "set()\n",
            "set()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h7Rvg6vWyYI",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmcu_t772CVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math \n",
        "# Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=200):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients by the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def gen_nopeek_mask(length):\n",
        "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -> w h')\n",
        "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -> w h')\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkJPanlrMuZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos_dropout, trans_dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed_src = nn.Embedding(vocab_size, d_model)\n",
        "        self.embed_tgt = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def init_embeddings(self, wv):\n",
        "        assert(self.embed_src.weight.data.shape == wv.shape)\n",
        "        assert(self.embed_tgt.weight.data.shape == wv.shape)\n",
        "        self.embed_src.weight.data = wv\n",
        "        self.embed_tgt.weight.data = wv\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_mask):\n",
        "        src = src.permute(1, 0)\n",
        "        tgt = tgt.permute(1, 0)\n",
        "        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model)) # why every time multiply? just init it somewhere\n",
        "        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,\n",
        "                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.fc(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JN7ZUbZAOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3eb435b2-82e1-46fe-d49b-8ea03bcbfa98"
      },
      "source": [
        "# init\n",
        "model = LanguageTransformer(lang.n_words, config['d_model'], config['nhead'], \n",
        "                            config['num_encoder_layers'],config['num_decoder_layers'],\n",
        "                            config['dim_feedforward'], lang.max_seq_length,\n",
        "                            config['pos_dropout'], config['trans_dropout'])\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_normal_(p)\n",
        "\n",
        "wv = torch.Tensor([word_init_vectors[lang.index2word[idx]] for idx in lang.index2word.keys()])\n",
        "model.init_embeddings(wv)\n",
        "\n",
        "model.to(device)\n",
        "# optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "# optimizer = Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09)\n",
        "optimizer = ScheduledOptim(\n",
        "    Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "    config['d_model'], config['n_warmup_steps'])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=lang.word2index['PAD'])\n",
        "\n",
        "\n",
        "\n",
        "# train\n",
        "epochs = 10\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "train_dataset = HeadlinesDataset(pairs[1024:], lang, batch_size=64) # config['batch_size'])\n",
        "test_dataset  = HeadlinesDataset(pairs[:1024], lang, batch_size=64) # config['batch_size'])\n",
        "\n",
        "for epoch_i in range(epochs):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    \n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    # for src, tgt, src_key_padding_mask, tgt_key_padding_mask in tqdm.tqdm(iter(train_loader)):\n",
        "    for src, tgt, src_key_padding_mask, tgt_key_padding_mask in iter(train_loader):\n",
        "        src, src_key_padding_mask = src[0].to(device), src_key_padding_mask[0].to(device) # to device\n",
        "        tgt, tgt_key_padding_mask = tgt[0].to(device), tgt_key_padding_mask[0].to(device) # to device\n",
        "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "        tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "        tgt_key_padding_mask_inp = tgt_key_padding_mask[:, :-1]\n",
        "        tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device) # also to device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask_inp, memory_key_padding_mask, tgt_mask)\n",
        "        loss = criterion(rearrange(output, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
        "        loss.backward()\n",
        "        # optimizer.step()\n",
        "        optimizer.step_and_update_lr()\n",
        "        train_loss += loss.item()\n",
        "    train_losses.append((epoch_i, train_loss))\n",
        "\n",
        "\n",
        "    test_loss = 0\n",
        "    model.eval()\n",
        "    # for src, tgt, src_key_padding_mask, tgt_key_padding_mask in tqdm.tqdm(iter(test_loader)):\n",
        "    for src, tgt, src_key_padding_mask, tgt_key_padding_mask in iter(test_loader):\n",
        "        with torch.no_grad():\n",
        "            src, src_key_padding_mask = src[0].to(device), src_key_padding_mask[0].to(device) # to device\n",
        "            tgt, tgt_key_padding_mask = tgt[0].to(device), tgt_key_padding_mask[0].to(device) # to device\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "            tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "            tgt_key_padding_mask_inp = tgt_key_padding_mask[:, :-1]\n",
        "            tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device) # also to device\n",
        "\n",
        "            output = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask_inp, memory_key_padding_mask, tgt_mask)\n",
        "            loss = criterion(rearrange(output, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
        "            test_loss += loss.item()\n",
        "    test_losses.append((epoch_i, test_loss))\n",
        "\n",
        "    print(f\"\\nEpoch: {epoch_i} Train loss: {train_loss} Test loss: {test_loss}\")\n",
        "    # break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0 Train loss: 1283.8974475860596 Test loss: 138.9708490371704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVSI7qYj0gds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI-3Spl-0gls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRDpuv3ezUT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MYZdOtCzUoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCJHC_udqSww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_, src_key_padding_mask_ = src[0].to(device), src_key_padding_mask[0].to(device)\n",
        "tgt_, tgt_key_padding_mask_ = tgt[0].to(device), tgt_key_padding_mask[0].to(device)\n",
        "memory_key_padding_mask = src_key_padding_mask_.clone()\n",
        "tgt_inp, tgt_out = tgt_[:, :-1], tgt_[:, 1:]\n",
        "tgt_key_padding_mask_inp = tgt_key_padding_mask_[:, :-1]\n",
        "tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvaPZRRvuOg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tgt_key_padding_mask_.shape)\n",
        "print(src_.shape)\n",
        "print(tgt_inp.shape)\n",
        "print(src_key_padding_mask_.shape)\n",
        "print(memory_key_padding_mask.shape)\n",
        "print(tgt_mask.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlsp_16Etdii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = model(src_, tgt_inp, src_key_padding_mask_, tgt_key_padding_mask_inp, memory_key_padding_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTX_erk1t2WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " torch.isnan(output).any()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5_9Agftv8Vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbyDvyKzwQyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwtgvBOdw6Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src[0][9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddF9kC1Uw0B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lang.tensor2text(tgt[0][9])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd-JydO0vuCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for row_i in range(src_.shape[0]):\n",
        "  print(row_i, src_[row_i][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEKnVJ-fp6Eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src, src_key_padding_mask = src[0].to(device), src_key_padding_mask[0].to(device) # to device\n",
        "tgt, tgt_key_padding_mask = tgt[0].to(device), tgt_key_padding_mask[0].to(device) # to device\n",
        "memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "tgt_key_padding_mask_inp = tgt_key_padding_mask[:, :-1]\n",
        "tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device) # also to device\n",
        "\n",
        "optimizer.zero_grad()\n",
        "output = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask_inp, memory_key_padding_mask, tgt_mask)\n",
        "loss = criterion(rearrange(output, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "train_loss += loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiETl8cebCGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask_inp, memory_key_padding_mask, tgt_mask)\n",
        "criterion(rearrange(output, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrOG2X88b8XL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_ = src.permute(1, 0)\n",
        "tgt_ = tgt_inp.permute(1, 0)\n",
        "# src_ = model.pos_enc(model.embed_src(src_) * math.sqrt(model.d_model)) # why every time multiply? just init it somewhere\n",
        "# tgt_ = model.pos_enc(model.embed_tgt(tgt_) * math.sqrt(model.d_model))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMlRQSWTcT7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_[:, 0].unsqueeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZd99ArrcC57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.embed_src(src_[:, 0].unsqueeze(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ei_HEKSelPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.embed_src(torch.Tensor([[lang.word2index['PAD']]]).long().to(device) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UklVda-bndB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,\n",
        "                          tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "output = output.permute(1, 0, 2)\n",
        "return self.fc(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03qw5a46bieg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcwjAlMA1y7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "global_embeddings = nn.Embedding(lang.n_words, config['d_model'])\n",
        "wv = torch.Tensor([word_init_vectors[lang.index2word[idx]] for idx in lang.index2word.keys()])\n",
        "global_embeddings.weight.data = wv\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=lang.word2index['PAD'])\n",
        "\n",
        "d_model = config['d_model']\n",
        "pos_dropout = config['pos_dropout']\n",
        "nhead = config['nhead']\n",
        "num_encoder_layers = config['num_encoder_layers']\n",
        "num_decoder_layers = config['num_decoder_layers']\n",
        "dim_feedforward = config['dim_feedforward']\n",
        "trans_dropout = config['trans_dropout']\n",
        "\n",
        "\n",
        "\n",
        "embed_src = global_embeddings\n",
        "embed_tgt = global_embeddings\n",
        "pos_enc = PositionalEncoding(d_model, pos_dropout, lang.max_seq_length)\n",
        "transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
        "fc = nn.Linear(d_model, lang.n_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHYTLN6Q55Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src, tgt, src_key_padding_mask, tgt_key_padding_mask =  next(iter(train_loader))\n",
        "src, src_key_padding_mask = src[0], src_key_padding_mask[0].to(device) # to device\n",
        "tgt, tgt_key_padding_mask = tgt[0], tgt_key_padding_mask[0].to(device) # to device\n",
        "memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "tgt_key_padding_mask_inp = tgt_key_padding_mask[:, :-1]\n",
        "tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device) # also to device\n",
        "\n",
        "print(src.shape)\n",
        "print(tgt.shape)\n",
        "print(src_key_padding_mask.shape)\n",
        "print(tgt_key_padding_mask.shape)\n",
        "print(tgt_inp.shape)\n",
        "print(tgt_out.shape)\n",
        "print(tgt_mask.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGmF1vKh7dCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = src.permute(1, 0)\n",
        "tgt_inp = tgt_inp.permute(1, 0)\n",
        "src = pos_enc(embed_src(src) * math.sqrt(d_model))\n",
        "tgt_inp = pos_enc(embed_tgt(tgt_inp) * math.sqrt(d_model))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1ovGU5J90GA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(src.shape, tgt_inp.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtXVOdxY-lfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(src.shape)\n",
        "print(tgt_inp.shape)\n",
        "print(tgt_mask.shape)\n",
        "print(src_key_padding_mask.shape)\n",
        "print(tgt_key_padding_mask_inp.shape)\n",
        "print(memory_key_padding_mask.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It8h9lga3Pii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = transformer(src, tgt_inp, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,\n",
        "                          tgt_key_padding_mask=tgt_key_padding_mask_inp, memory_key_padding_mask=memory_key_padding_mask)\n",
        "output = output.permute(1, 0, 2)\n",
        "output = fc(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0-s6h7X9te1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = criterion(rearrange(output, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_5b9bxKAJSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0LtWb5iW1A8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class LanguageTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos_dropout, trans_dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed_src = nn.Embedding(vocab_size, d_model)\n",
        "        self.embed_tgt = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_mask):\n",
        "        # src = rearrange(src, 'n s -> s n')\n",
        "        src = src.permute(1, 0)\n",
        "        # tgt = rearrange(tgt, 'n t -> t n')\n",
        "        tgt = tgt.permute(1, 0)\n",
        "        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model)) # why every time multiply? just init it somewhere\n",
        "        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,\n",
        "                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        # output = rearrange(output, 't n e -> n t e')\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.fc(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unw4BYAFZAVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        total_step += 1\n",
        "\n",
        "        src, src_key_padding_mask = src[0].to('cuda'), src_key_padding_mask[0].to('cuda')\n",
        "        tgt, tgt_key_padding_mask = tgt[0].to('cuda'), tgt_key_padding_mask[0].to('cuda')\n",
        "\n",
        "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "        tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "        tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask[:, :-1], memory_key_padding_mask, tgt_mask)\n",
        "        loss = criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step_and_update_lr()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        train_losses.append((step, loss.item()))\n",
        "        pbar.update(1)\n",
        "        if step % print_every == print_every - 1:\n",
        "            pbar.close()\n",
        "            print(f'Epoch [{epoch + 1} / {num_epochs}] \\t Step [{step + 1} / {len(train_loader)}] \\t '\n",
        "                  f'Train Loss: {total_loss / print_every}')\n",
        "            total_loss = 0\n",
        "\n",
        "            pbar = tqdm(total=print_every, leave=False)\n",
        "\n",
        "    pbar.close()\n",
        "    val_loss = validate(valid_loader, model, criterion)\n",
        "    val_losses.append((total_step, val_loss))\n",
        "    if val_loss < lowest_val:\n",
        "        lowest_val = val_loss\n",
        "        torch.save(model, 'output/transformer.pth')\n",
        "    print(f'Val Loss: {val_loss}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5m8S0o8ZAiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91eDhSkXFFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWMTJGMVeO0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ParallelLanguageDataset(Dataset):\n",
        "    def __init__(self, data_path_1, data_path_2, num_tokens, max_seq_length):\n",
        "        self.num_tokens = num_tokens\n",
        "        self.data_1, self.data_2, self.data_lengths = load_data(data_path_1, data_path_2, max_seq_length)\n",
        "\n",
        "        self.batches = gen_batches(num_tokens, self.data_lengths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, src_mask = getitem(idx, self.data_1, self.batches, True)\n",
        "        tgt, tgt_mask = getitem(idx, self.data_2, self.batches, False)\n",
        "\n",
        "        return src, src_mask, tgt, tgt_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def shuffle_batches(self):\n",
        "        self.batches = gen_batches(self.num_tokens, self.data_lengths)\n",
        "\n",
        "\n",
        "def gen_batches(num_tokens, data_lengths):\n",
        "    # Shuffle all the indices\n",
        "    for k, v in data_lengths.items():\n",
        "        random.shuffle(v)\n",
        "\n",
        "    batches = []\n",
        "    prev_tokens_in_batch = 1e10\n",
        "    for k in sorted(data_lengths):\n",
        "        v = data_lengths[k]\n",
        "        total_tokens = (k[0] + k[1]) * len(v)\n",
        "\n",
        "        while total_tokens > 0:\n",
        "            tokens_in_batch = min(total_tokens, num_tokens) - min(total_tokens, num_tokens) % (k[0] + k[1])\n",
        "            sentences_in_batch = tokens_in_batch // (k[0] + k[1])\n",
        "\n",
        "            # Combine with previous batch?\n",
        "            if tokens_in_batch + prev_tokens_in_batch <= num_tokens:\n",
        "                batches[-1].extend(v[:sentences_in_batch])\n",
        "                prev_tokens_in_batch += tokens_in_batch\n",
        "            else:\n",
        "                batches.append(v[:sentences_in_batch])\n",
        "                prev_tokens_in_batch = tokens_in_batch\n",
        "            v = v[sentences_in_batch:]\n",
        "\n",
        "            total_tokens = (k[0] + k[1]) * len(v)\n",
        "    return batches\n",
        "\n",
        "\n",
        "def load_data(data_path_1, data_path_2, max_seq_length):\n",
        "    with open(data_path_1, 'rb') as f:\n",
        "        data_1 = pickle.load(f)\n",
        "    with open(data_path_2, 'rb') as f:\n",
        "        data_2 = pickle.load(f)\n",
        "\n",
        "    data_lengths = {}\n",
        "    for i, (str_1, str_2) in enumerate(zip(data_1, data_2)):\n",
        "        if 0 < len(str_1) <= max_seq_length and 0 < len(str_2) <= max_seq_length - 2:\n",
        "            if (len(str_1), len(str_2)) in data_lengths:\n",
        "                data_lengths[(len(str_1), len(str_2))].append(i)\n",
        "            else:\n",
        "                data_lengths[(len(str_1), len(str_2))] = [i]\n",
        "    return data_1, data_2, data_lengths\n",
        "\n",
        "\n",
        "def getitem(idx, data, batches, src):\n",
        "    sentence_indices = batches[idx]\n",
        "    if src:\n",
        "        batch = [data[i] for i in sentence_indices]\n",
        "    else:\n",
        "        batch = [[2] + data[i] + [3] for i in sentence_indices]\n",
        "\n",
        "    seq_length = 0\n",
        "    for sentence in batch:\n",
        "        if len(sentence) > seq_length:\n",
        "            seq_length = len(sentence)\n",
        "\n",
        "    masks = []\n",
        "    for i, sentence in enumerate(batch):\n",
        "        masks.append([False for _ in range(len(sentence))] + [True for _ in range(seq_length - len(sentence))])\n",
        "        batch[i] = sentence + [0 for _ in range(seq_length - len(sentence))]\n",
        "\n",
        "    return np.array(batch), np.array(masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4IQmfdDeCPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients by the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV2jC2Isd8H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, valid_loader, model, optim, criterion, num_epochs):\n",
        "    print_every = 500\n",
        "    model.train()\n",
        "\n",
        "    lowest_val = 1e9\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    total_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm(total=print_every, leave=False)\n",
        "        total_loss = 0\n",
        "\n",
        "        train_loader.dataset.shuffle_batches()\n",
        "        for step, (src, src_key_padding_mask, tgt, tgt_key_padding_mask) in enumerate(iter(train_loader)):\n",
        "            total_step += 1\n",
        "\n",
        "            src, src_key_padding_mask = src[0].to('cuda'), src_key_padding_mask[0].to('cuda')\n",
        "            tgt, tgt_key_padding_mask = tgt[0].to('cuda'), tgt_key_padding_mask[0].to('cuda')\n",
        "\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "            tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "            tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')\n",
        "\n",
        "            optim.zero_grad()\n",
        "            outputs = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask[:, :-1], memory_key_padding_mask, tgt_mask)\n",
        "            loss = criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step_and_update_lr()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            train_losses.append((step, loss.item()))\n",
        "            pbar.update(1)\n",
        "            if step % print_every == print_every - 1:\n",
        "                pbar.close()\n",
        "                print(f'Epoch [{epoch + 1} / {num_epochs}] \\t Step [{step + 1} / {len(train_loader)}] \\t '\n",
        "                      f'Train Loss: {total_loss / print_every}')\n",
        "                total_loss = 0\n",
        "\n",
        "                pbar = tqdm(total=print_every, leave=False)\n",
        "\n",
        "        pbar.close()\n",
        "        val_loss = validate(valid_loader, model, criterion)\n",
        "        val_losses.append((total_step, val_loss))\n",
        "        if val_loss < lowest_val:\n",
        "            lowest_val = val_loss\n",
        "            torch.save(model, 'output/transformer.pth')\n",
        "        print(f'Val Loss: {val_loss}')\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def validate(valid_loader, model, criterion):\n",
        "    pbar = tqdm(total=len(iter(valid_loader)), leave=False)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    for src, src_key_padding_mask, tgt, tgt_key_padding_mask in iter(valid_loader):\n",
        "        with torch.no_grad():\n",
        "            src, src_key_padding_mask = src[0].to('cuda'), src_key_padding_mask[0].to('cuda')\n",
        "            tgt, tgt_key_padding_mask = tgt[0].to('cuda'), tgt_key_padding_mask[0].to('cuda')\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "            tgt_inp = tgt[:, :-1]\n",
        "            tgt_out = tgt[:, 1:].contiguous()\n",
        "            tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')\n",
        "\n",
        "            outputs = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask[:, :-1], memory_key_padding_mask, tgt_mask)\n",
        "            loss = criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    model.train()\n",
        "    return total_loss / len(valid_loader)\n",
        "\n",
        "\n",
        "def gen_nopeek_mask(length):\n",
        "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -> w h')\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwUsgjTygwm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kwargs = {\n",
        "    'num_epochs': 20,\n",
        "    'max_seq_length': 96,\n",
        "    'num_tokens': 2000,\n",
        "    'vocab_size': 10000+4,\n",
        "    'd_model': 512,\n",
        "    'num_encoder_layers': 6,\n",
        "    'num_decoder_layers': 6,\n",
        "    'dim_feedforward': 2048,\n",
        "    'nhead': 8,\n",
        "    'pos_dropout': 0.1,\n",
        "    'trans_dropout': 0.1,\n",
        "    'n_warmup_steps': 4000\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RspHKWOthXrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = ParallelLanguageDataset(root + 'data/processed/en/train.pkl',\n",
        "                                        root + 'data/processed/fr/train.pkl',\n",
        "                                        kwargs['num_tokens'], kwargs['max_seq_length'])\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_dataset = ParallelLanguageDataset(root + 'data/processed/en/val.pkl',\n",
        "                                        root + 'data/processed/fr/val.pkl',\n",
        "                                        kwargs['num_tokens'], kwargs['max_seq_length'])\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnP2tyOGhgMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src, src_key_padding_mask, tgt, tgt_key_padding_mask = train_dataset[100]\n",
        "print(src.shape)\n",
        "print(tgt.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S7ltY2jrR2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnA9YQ9Mgvei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LanguageTransformer(kwargs['vocab_size'], kwargs['d_model'], kwargs['nhead'], kwargs['num_encoder_layers'],\n",
        "                            kwargs['num_decoder_layers'], kwargs['dim_feedforward'], kwargs['max_seq_length'],\n",
        "                            kwargs['pos_dropout'], kwargs['trans_dropout']).to('cuda')\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_normal_(p)\n",
        "\n",
        "optim = ScheduledOptim(\n",
        "    Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "    kwargs['d_model'], kwargs['n_warmup_steps'])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3FvWIMWpRpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_every = 500\n",
        "model.train()\n",
        "\n",
        "lowest_val = 1e9\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "total_step = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND4J-jJbxjCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for epoch in range(kwargs['num_epochs']):\n",
        "epoch = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOkl_I_exku2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pbar = tqdm(total=print_every, leave=False)\n",
        "total_loss = 0\n",
        "train_loader.dataset.shuffle_batches()\n",
        "\n",
        "# for step, (src, src_key_padding_mask, tgt, tgt_key_padding_mask) in enumerate(iter(train_loader)):\n",
        "step, (src, src_key_padding_mask, tgt, tgt_key_padding_mask) = next(enumerate(iter(train_loader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze5Xan60zjN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(src.shape, tgt.shape)\n",
        "\n",
        "total_step += 1\n",
        "\n",
        "src, src_key_padding_mask = src[0].to('cuda'), src_key_padding_mask[0].to('cuda')\n",
        "tgt, tgt_key_padding_mask = tgt[0].to('cuda'), tgt_key_padding_mask[0].to('cuda')\n",
        "\n",
        "print(src.shape)\n",
        "print(tgt.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUjqMCvt3U_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memory_key_padding_mask = src_key_padding_mask.clone()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5kF69V45F0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:] # shift for 1 position"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOf5mPfJ5HYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tgt_inp.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yJfGknU5Lh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tgt_out.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2aq9-DW57CB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_nopeek_mask(length):\n",
        "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -> w h')\n",
        "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -> w h')\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "gen_nopeek_mask(tgt_inp.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2MF0MHn8VKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFevNzG_yl_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optim.zero_grad()\n",
        "outputs = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask[:, :-1], memory_key_padding_mask, tgt_mask)\n",
        "loss = criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2mjXkUB8lxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Wo1KoS8d1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rearrange(outputs, 'b t v -> (b t) v').shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA9eENjWI5c9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rearrange(tgt_out, 'b o -> (b o)').shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aug6U-WOAzcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rearrange(np.array([]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CFZCC2f83Nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNdLwMFc8cjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss.backward()\n",
        "optim.step_and_update_lr()\n",
        "\n",
        "total_loss += loss.item()\n",
        "train_losses.append((step, loss.item()))\n",
        "pbar.update(1)\n",
        "if step % print_every == print_every - 1:\n",
        "    pbar.close()\n",
        "    print(f'Epoch [{epoch + 1} / {num_epochs}] \\t Step [{step + 1} / {len(train_loader)}] \\t '\n",
        "          f'Train Loss: {total_loss / print_every}')\n",
        "    total_loss = 0\n",
        "\n",
        "    pbar = tqdm(total=print_every, leave=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhUT2foyvnFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pbar.close()\n",
        "val_loss = validate(valid_loader, model, criterion)\n",
        "val_losses.append((total_step, val_loss))\n",
        "if val_loss < lowest_val:\n",
        "    lowest_val = val_loss\n",
        "    torch.save(model, 'output/transformer.pth')\n",
        "print(f'Val Loss: {val_loss}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}