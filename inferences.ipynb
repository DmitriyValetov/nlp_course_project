{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inferences.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP69jDMoGFwCfJ7EYMbQab+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitriyValetov/nlp_course_project/blob/master/inferences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4r5ZXvKsFx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import choice, randint\n",
        "\n",
        "np.random.seed(0)\n",
        "emb_size = 5  # 0, 1, 2, 3, 4\n",
        "batch_size = 2\n",
        "x1_lens = randint(1, 4, batch_size)\n",
        "print(x1_lens)\n",
        "max_len = np.max(x1_lens)\n",
        "print(max_len)\n",
        "x1_base = randint(1, 5, (batch_size, 4))\n",
        "bx1 = np.array([[2] + list(choice([1, 4], x)) + [3] + [0] * (max_len - x) for x in x1_lens])\n",
        "print(bx1)\n",
        "\n",
        "def predict(bx1, bx2):\n",
        "  np.random.seed(0)\n",
        "  batch_size, x2_len = bx2.shape\n",
        "  a = np.random.pareto(2, (x2_len, batch_size, emb_size))  # activations\n",
        "  a = a.swapaxes(0, 1)  # right behaviour\n",
        "  # print(a)\n",
        "  s = np.exp(a)/np.sum(np.exp(a), axis=2, keepdims=True)  # softmax\n",
        "  # print(s)\n",
        "  # print(np.sum(s, axis=2))\n",
        "  by2p = np.log(s)  # log softmax\n",
        "  return by2p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJItFZCZW5vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy(bx1, sos=2, eos=3, max_len=10):\n",
        "  bx2 = np.full((batch_size, 1), sos)  # batch with <sos> token\n",
        "  # stop when predictions len > max_len or all have <eos> token\n",
        "  while 1 + bx2.shape[1] < max_len and not np.all(np.any(bx2 == eos, axis=1)):\n",
        "    # print(bx2)\n",
        "    by2p = predict(bx1, bx2)\n",
        "    lp = by2p[:,-1,:]  # last prediction\n",
        "    # print(by2)\n",
        "    # print(lp)\n",
        "    next_bx2 = np.argmax(lp, axis=1).reshape((lp.shape[0], 1))\n",
        "    # print(next_bx2)\n",
        "    bx2 = np.concatenate((bx2, next_bx2), axis=1)\n",
        "    # print(bx2)\n",
        "    # print(bx2 == eos)\n",
        "    # print(np.all(np.any(bx2 == eos, axis=1)))\n",
        "  return by2p\n",
        "\n",
        "def greedy_many_to_one(bx1, sos=2, eos=3, max_len=10, reduction=np.sum):\n",
        "  bx2 = np.full((bx1.shape[0], 1), sos)  # batch with <sos> token\n",
        "  # stop when predictions len > max_len or all have <eos> token\n",
        "  while 1 + bx2.shape[1] < max_len and not np.all(np.any(bx2 == eos, axis=1)):\n",
        "    print(bx2)\n",
        "    by2p = predict(bx1, bx2)\n",
        "    lp = by2p[:,-1,:]  # last prediction\n",
        "    print(lp)\n",
        "    lp_red = reduction(lp, axis=0)\n",
        "    print(lp_red)\n",
        "    # print(by2)\n",
        "    # print(lp)\n",
        "    # next_bx2 = np.argmax(lp, axis=1).reshape((lp.shape[0], 1))\n",
        "    next_x2 = np.argmax(lp_red, axis=0)\n",
        "    # print(next_x2)\n",
        "    next_bx2 = np.full((lp.shape[0], 1), next_x2)\n",
        "    # print(next_bx2)\n",
        "    bx2 = np.concatenate((bx2, next_bx2), axis=1)\n",
        "    # print(bx2)\n",
        "    # print(bx2 == eos)\n",
        "    # print(np.all(np.any(bx2 == eos, axis=1)))\n",
        "  return by2p\n",
        "\n",
        "def forced(bx1, bx2):\n",
        "  by2p = predict(bx1, bx2)\n",
        "  return by2p\n",
        "\n",
        "def many_to_one(by2p, reduction=np.sum):\n",
        "  y2p = np.sum(by2p[:,:-1,:], axis=0)\n",
        "  y2 = np.argmax(y2p, axis=1)\n",
        "  return y2\n",
        "\n",
        "# by2p = greedy_many_to_one(bx1, reduction=np.sum)\n",
        "# print(by2p)\n",
        "# by2p = greedy_many_to_one(bx1, reduction=np.max)\n",
        "# print(by2p)\n",
        "# by2p = greedy_many_to_one(bx1, reduction=np.mean)\n",
        "# print(by2p)\n",
        "# y2 = np.argmax(np.sum(by2p[:,:-1,:], axis=0), axis=1)\n",
        "# print(y2)\n",
        "# print(many_to_one_sum(by2p))\n",
        "# by2p = greedy(bx1)\n",
        "# print(by2p)\n",
        "# by2 = np.argmax(by2p[:,:-1,:], axis=2)\n",
        "# print(by2)\n",
        "# print(many_to_one(by2p, np.sum))\n",
        "# print(many_to_one(by2p, np.max))\n",
        "# print(many_to_one(by2p, np.mean))\n",
        "# forced\n",
        "# bx2 = np.concatenate((np.full((batch_size, 1), 2), by2), axis=1)\n",
        "# print(bx2)\n",
        "# by2p = forced(bx1, bx2)\n",
        "# # print(bpy2)\n",
        "# by2 = np.argmax(by2p[:,:-1,:], axis=2)\n",
        "# print(by2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35vwuEvbsADZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "569d7709-2b13-40d4-e6dc-e747c62f0ad8"
      },
      "source": [
        "# TODO do many_to_one search with batch_reduction\n",
        "# TODO try Viterbi algorithm for best beam searching (like beam search but in reversed direction)\n",
        "# TODO maybe try spectral beams? (With multiple reductions + Viterbi)\n",
        "\n",
        "def beam(bx1, sos=2, eos=3, max_len=5,\n",
        "         beam_width=2, beam_depth=2, depth_reduction=np.sum):\n",
        "  batch_size = bx1.shape[0]\n",
        "  bx2 = np.full((batch_size, 1), sos)  # base batch with <sos> token\n",
        "  # stop when predictions len > max_len or all have <eos> token\n",
        "  while 1 + bx2.shape[1] < max_len and not np.all(np.any(bx2 == eos, axis=1)):\n",
        "    bx2t = bx2.copy()  # temporal batch bx2\n",
        "    for i in range(beam_depth):\n",
        "      by2p = predict(bx1, bx2t)  # all predictions\n",
        "      lp = by2p[:,-1,:]  # last prediction\n",
        "      next_bx2 = np.argsort(-lp, axis=1)[:,:beam_width]  # top \"beam_width\" last predictions\n",
        "      bx2t = np.repeat(bx2t, beam_width, axis=0)  # multiply batch_size by beam_width\n",
        "      next_bx2 = next_bx2.reshape(batch_size*beam_width**(i+1), 1)  # to new batch_size\n",
        "      bx2t = np.concatenate((bx2t, next_bx2), axis=1)  # update batch\n",
        "    # Prediction with temporal batch\n",
        "    by2p = predict(bx1, bx2t)  # all predictions by last temporal batch\n",
        "    by2p = by2p[:,:-1]  # remove last prediction\n",
        "    bx2t = bx2t[:,1:]  # remove <sos> from temporal batch\n",
        "    # Best beams searching\n",
        "    by2p = np.take_along_axis(by2p, bx2t[:,np.newaxis], axis=2)  # beams transitions predictions\n",
        "    by2p = by2p.reshape(batch_size, beam_width**beam_depth, -1)  # reshape to base batch shape (like)\n",
        "    by2p_red = depth_reduction(by2p, axis=2)  # beams reduction, i.e. giving them scores\n",
        "    best_beams = np.argmax(by2p_red, axis=1)  # get indices of the best beams\n",
        "    # Base batch updating\n",
        "    bx2t = bx2t.reshape(batch_size, beam_width**beam_depth, -1)  # reshape to base batch shape (like)\n",
        "    next_bx2 = np.take_along_axis(bx2t, best_beams[:, np.newaxis, np.newaxis], axis=1)  # get best beams\n",
        "    next_bx2 = next_bx2.reshape(batch_size, -1)  # to base batch_size\n",
        "    bx2 = np.concatenate((bx2, next_bx2), axis=1)  # update base batch\n",
        "  return bx2\n",
        "\n",
        "bx2 = beam(bx1, depth_reduction=np.sum)\n",
        "print(bx2)\n",
        "bx2 = beam(bx1, depth_reduction=np.max)\n",
        "print(bx2)\n",
        "bx2 = beam(bx1, depth_reduction=np.mean)\n",
        "print(bx2)"
      ],
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 2 2 2 2 0 2]\n",
            " [2 3 3 3 3 0 1]]\n",
            "[[2 2 2 2 2 1 3]\n",
            " [2 2 3 2 3 2 2]]\n",
            "[[2 2 2 2 2 0 2]\n",
            " [2 3 3 3 3 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfgwGkcxqzZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "10227cc4-09f3-45b3-8380-84b390287b24"
      },
      "source": [
        "import torch\n",
        "bt = bx1[:,1:]  # without <sos>\n",
        "print(bt)\n",
        "print(bt[bt != 0])\n",
        "bp = by2[:,:-1,:]  # without last prediction\n",
        "print(bp)\n",
        "print(bt != 0)\n",
        "print(bp[bt != 0])\n",
        "\n",
        "bp, bt = torch.tensor(bp), torch.tensor(bt)\n",
        "loss = torch.nn.NLLLoss()\n",
        "print(loss(bp[bt != 0], bt[bt != 0]))\n",
        "\n",
        "loss = torch.nn.NLLLoss(ignore_index=0)\n",
        "print(loss(torch.flatten(bp, 0, 1), torch.flatten(bt, 0)))\n",
        "\n",
        "loss = 0\n",
        "cnt = 0\n",
        "for i, t in enumerate(bt):\n",
        "  for j, l in enumerate(t):\n",
        "    print(l)\n",
        "    print(bp[i,j,l])\n",
        "    loss += bp[i,j,l].item()\n",
        "    cnt += 1\n",
        "print(loss, loss/cnt)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5 3 0]\n",
            " [1 5 3]]\n",
            "[5 3 1 5 3]\n",
            "[[[-1.89006574 -1.50502211 -1.79218746 -1.89650799 -2.06159531\n",
            "   -1.69833605]\n",
            "  [-4.11028414 -2.40401057 -0.19777221 -4.17018005 -3.25252363\n",
            "   -3.98678273]\n",
            "  [-2.63134807 -0.48678304 -3.11534831 -3.10624282 -3.14261297\n",
            "   -1.70861462]]\n",
            "\n",
            " [[-3.42675656 -2.82523264 -3.41128033 -0.24047856 -3.04555381\n",
            "   -3.18465323]\n",
            "  [-2.15389649 -1.21536353 -1.96396499 -1.79775144 -2.31043832\n",
            "   -1.70277709]\n",
            "  [-2.90484114 -2.89473326 -0.29414425 -2.73762603 -3.26092274\n",
            "   -3.17766349]]]\n",
            "[[ True  True False]\n",
            " [ True  True  True]]\n",
            "[[-1.89006574 -1.50502211 -1.79218746 -1.89650799 -2.06159531 -1.69833605]\n",
            " [-4.11028414 -2.40401057 -0.19777221 -4.17018005 -3.25252363 -3.98678273]\n",
            " [-3.42675656 -2.82523264 -3.41128033 -0.24047856 -3.04555381 -3.18465323]\n",
            " [-2.15389649 -1.21536353 -1.96396499 -1.79775144 -2.31043832 -1.70277709]\n",
            " [-2.90484114 -2.89473326 -0.29414425 -2.73762603 -3.26092274 -3.17766349]]\n",
            "tensor(2.6268, dtype=torch.float64)\n",
            "tensor(2.6268, dtype=torch.float64)\n",
            "tensor(5)\n",
            "tensor(-1.6983, dtype=torch.float64)\n",
            "tensor(3)\n",
            "tensor(-4.1702, dtype=torch.float64)\n",
            "tensor(0)\n",
            "tensor(-2.6313, dtype=torch.float64)\n",
            "tensor(1)\n",
            "tensor(-2.8252, dtype=torch.float64)\n",
            "tensor(5)\n",
            "tensor(-1.7028, dtype=torch.float64)\n",
            "tensor(3)\n",
            "tensor(-2.7376, dtype=torch.float64)\n",
            "-15.765499936791546 -2.627583322798591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND82pLbZmGj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}