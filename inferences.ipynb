{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inferences.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMD39js60uvz7fwu/RF1K8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitriyValetov/nlp_course_project/blob/master/inferences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4r5ZXvKsFx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "bcf4a75a-ef4c-453b-8aa3-d8162f5a46f7"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import choice, randint\n",
        "\n",
        "np.random.seed(0)\n",
        "pad, unk, sos, eos = 0, 1, 2, 3\n",
        "emb_size = 5  # 0, 1, 2, 3, 4\n",
        "batch_size = 3\n",
        "x1_lens = randint(1, 4, batch_size)\n",
        "print(x1_lens)\n",
        "max_len = np.max(x1_lens)\n",
        "print(max_len)\n",
        "x1_base = randint(1, 5, (batch_size, 4))\n",
        "bx1 = np.array([[2] + list(choice([1, 4], x)) + [3] + [0] * (max_len - x) for x in x1_lens])\n",
        "print(bx1)\n",
        "\n",
        "def predict(bx1, bx2, emb_size=5):\n",
        "  np.random.seed(0)\n",
        "  batch_size, len_x2 = bx2.shape\n",
        "  a = np.random.pareto(2, (len_x2, batch_size, emb_size))  # activations\n",
        "  a = a.swapaxes(0, 1)  # for consitency\n",
        "  s = np.exp(a)/np.sum(np.exp(a), axis=2, keepdims=True)  # softmax\n",
        "  by2p = np.log(s)  # log softmax\n",
        "  return by2p\n",
        "\n",
        "import torch\n",
        "from torch.distributions.pareto import Pareto\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def torch_predict(bx1, bx2, emb_size=5):\n",
        "  torch.manual_seed(0)\n",
        "  batch_size, len_x2 = bx2.size()\n",
        "  p = Pareto(torch.tensor([2.0]), torch.tensor([1.0]))  # Pareto\n",
        "  a = p.sample((len_x2, batch_size, emb_size)).squeeze(-1)  # activations\n",
        "  a = a.permute(1, 0, 2)\n",
        "  by2p = F.log_softmax(a, 2)  # log softmax predictions\n",
        "  return by2p\n",
        "\n",
        "bx1 = torch.tensor(bx1)\n",
        "bx2 = torch.full((batch_size, 1), 2)\n",
        "torch_predict(bx1, bx2)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 1]\n",
            "2\n",
            "[[2 1 3 0]\n",
            " [2 1 1 3]\n",
            " [2 4 3 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000e+00, -5.9940e+01, -6.3085e+01, -4.1549e+01, -6.1150e+01]],\n",
              "\n",
              "        [[-1.1626e-02, -7.1529e+00, -6.5056e+00, -4.8106e+00, -6.7779e+00]],\n",
              "\n",
              "        [[-2.1067e+00, -3.3326e+00, -2.6601e+00, -4.7244e-01, -1.9022e+00]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJItFZCZW5vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy(bx1, sos=2, eos=3, max_len=10):\n",
        "  bx2 = np.full((batch_size, 1), sos)  # batch with <sos> token\n",
        "  # stop when predictions len > max_len or all have <eos> token\n",
        "  while 1 + bx2.shape[1] < max_len and not np.all(np.any(bx2 == eos, axis=1)):\n",
        "    # print(bx2)\n",
        "    by2p = predict(bx1, bx2)\n",
        "    lp = by2p[:,-1,:]  # last prediction\n",
        "    # print(by2)\n",
        "    # print(lp)\n",
        "    next_bx2 = np.argmax(lp, axis=1).reshape((lp.shape[0], 1))\n",
        "    # print(next_bx2)\n",
        "    bx2 = np.concatenate((bx2, next_bx2), axis=1)\n",
        "    # print(bx2)\n",
        "    # print(bx2 == eos)\n",
        "    # print(np.all(np.any(bx2 == eos, axis=1)))\n",
        "  return by2p\n",
        "\n",
        "def greedy_many_to_one(bx1, sos=2, eos=3, max_len=10, reduction=np.sum):\n",
        "  bx2 = np.full((bx1.shape[0], 1), sos)  # batch with <sos> token\n",
        "  # stop when predictions len > max_len or all have <eos> token\n",
        "  while 1 + bx2.shape[1] < max_len and not np.all(np.any(bx2 == eos, axis=1)):\n",
        "    print(bx2)\n",
        "    by2p = predict(bx1, bx2)\n",
        "    lp = by2p[:,-1,:]  # last prediction\n",
        "    print(lp)\n",
        "    lp_red = reduction(lp, axis=0)\n",
        "    print(lp_red)\n",
        "    # print(by2)\n",
        "    # print(lp)\n",
        "    # next_bx2 = np.argmax(lp, axis=1).reshape((lp.shape[0], 1))\n",
        "    next_x2 = np.argmax(lp_red, axis=0)\n",
        "    # print(next_x2)\n",
        "    next_bx2 = np.full((lp.shape[0], 1), next_x2)\n",
        "    # print(next_bx2)\n",
        "    bx2 = np.concatenate((bx2, next_bx2), axis=1)\n",
        "    # print(bx2)\n",
        "    # print(bx2 == eos)\n",
        "    # print(np.all(np.any(bx2 == eos, axis=1)))\n",
        "  return by2p\n",
        "\n",
        "def forced(bx1, bx2):\n",
        "  by2p = predict(bx1, bx2)\n",
        "  return by2p\n",
        "\n",
        "def many_to_one(by2p, reduction=np.sum):\n",
        "  y2p = np.sum(by2p[:,:-1,:], axis=0)\n",
        "  y2 = np.argmax(y2p, axis=1)\n",
        "  return y2\n",
        "\n",
        "# by2p = greedy_many_to_one(bx1, reduction=np.sum)\n",
        "# print(by2p)\n",
        "# by2p = greedy_many_to_one(bx1, reduction=np.max)\n",
        "# print(by2p)\n",
        "# by2p = greedy_many_to_one(bx1, reduction=np.mean)\n",
        "# print(by2p)\n",
        "# y2 = np.argmax(np.sum(by2p[:,:-1,:], axis=0), axis=1)\n",
        "# print(y2)\n",
        "# print(many_to_one_sum(by2p))\n",
        "# by2p = greedy(bx1)\n",
        "# print(by2p)\n",
        "# by2 = np.argmax(by2p[:,:-1,:], axis=2)\n",
        "# print(by2)\n",
        "# print(many_to_one(by2p, np.sum))\n",
        "# print(many_to_one(by2p, np.max))\n",
        "# print(many_to_one(by2p, np.mean))\n",
        "# forced\n",
        "# bx2 = np.concatenate((np.full((batch_size, 1), 2), by2), axis=1)\n",
        "# print(bx2)\n",
        "# by2p = forced(bx1, bx2)\n",
        "# # print(bpy2)\n",
        "# by2 = np.argmax(by2p[:,:-1,:], axis=2)\n",
        "# print(by2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35vwuEvbsADZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "d884cbcb-dce2-4921-f446-5fd2b2b77e2f"
      },
      "source": [
        "# TODO do many_to_one search with batch_reduction\n",
        "# TODO try Viterbi algorithm for best beam searching (like beam search but in reversed direction)\n",
        "# TODO maybe try spectral beams? (With multiple reductions + Viterbi)\n",
        "# bx1 - encoder batch input\n",
        "# bx2 - decoder batch input\n",
        "# by2p - encoder batch predictions\n",
        "\n",
        "def beam(bx1, sos=2, eos=3, max_len=6,\n",
        "         beam_width=2, beam_depth=2, depth_reduction=np.sum):\n",
        "  batch_size = bx1.shape[0]\n",
        "  bx2 = np.full((batch_size, 1), sos)  # base batch with <sos> token\n",
        "  cur_len = bx2.shape[1] - 1  # without <sos>\n",
        "  # stop when cur_len > max_len or all predictions have <eos> token\n",
        "  while cur_len < max_len and not np.all(np.any(bx2 == eos, axis=1)):\n",
        "    bx2t = bx2.copy()  # temporal batch bx2\n",
        "    for i in range(beam_depth):\n",
        "      by2p = predict(bx1, bx2t)  # all predictions\n",
        "      lp = by2p[:,-1,:]  # last prediction\n",
        "      next_bx2 = np.argsort(-lp, axis=1)[:,:beam_width]  # top \"beam_width\" last predictions\n",
        "      bx2t = np.repeat(bx2t, beam_width, axis=0)  # multiply batch_size by beam_width\n",
        "      next_bx2 = next_bx2.reshape(batch_size*beam_width**(i+1), 1)  # to new batch_size\n",
        "      bx2t = np.concatenate((bx2t, next_bx2), axis=1)  # update batch\n",
        "    # Prediction with temporal batch\n",
        "    by2p = predict(bx1, bx2t)  # all predictions by last temporal batch\n",
        "    by2p = by2p[:,cur_len:-1]  # [base part:-last]  # remove old predictions\n",
        "    bx2t = bx2t[:,cur_len+1:]  # [base part + <sos>:]  # remove old labels\n",
        "    # Best beams searching\n",
        "    by2p = np.take_along_axis(by2p, bx2t[:,np.newaxis], axis=2)  # beams transitions predictions\n",
        "    by2p = by2p.reshape(batch_size, beam_width**beam_depth, -1)  # reshape to base batch shape (like)\n",
        "    by2p_red = depth_reduction(by2p, axis=2)  # beams reduction, i.e. giving them scores\n",
        "    best_beams = np.argmax(by2p_red, axis=1)  # get indices of the best beams\n",
        "    # Base batch updating\n",
        "    bx2t = bx2t.reshape(batch_size, beam_width**beam_depth, -1)  # reshape to base batch shape (like)\n",
        "    bx2t = np.take_along_axis(bx2t, best_beams[:, np.newaxis, np.newaxis], axis=1)  # get best beams\n",
        "    bx2t = bx2t.reshape(batch_size, -1)  # to new labels to base batch_size\n",
        "    bx2 = np.concatenate((bx2, bx2t), axis=1)  # update base batch\n",
        "    cur_len = bx2.shape[1] - 1  # without <sos>\n",
        "  if bx2.shape[1] - 1 > max_len:  # cut to max_len\n",
        "    bx2 = bx2[:max_len + 1]  # <sos> included\n",
        "  return bx2\n",
        "\n",
        "bx2 = beam(bx1, depth_reduction=np.sum)\n",
        "print(bx2)\n",
        "bx2 = beam(bx1, depth_reduction=np.mean)\n",
        "print(bx2)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 2 3 0 0 3 1]\n",
            " [2 2 3 1 4 1 0]\n",
            " [2 0 1 2 4 4 1]\n",
            " [2 2 3 3 4 3 2]\n",
            " [2 1 3 2 3 2 2]]\n",
            "[[2 2 3 0 0 3 1]\n",
            " [2 2 3 1 4 1 0]\n",
            " [2 0 1 2 4 4 1]\n",
            " [2 2 3 3 4 3 2]\n",
            " [2 1 3 2 3 2 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVi25jHphkjG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "fabec289-d8e9-4d53-ab15-153ab9ff5a51"
      },
      "source": [
        "# TODO do many_to_one search with batch_reduction\n",
        "# TODO try Viterbi algorithm for best beam searching (like beam search but in reversed direction)\n",
        "# TODO maybe try spectral beams? (With multiple reductions + Viterbi)\n",
        "# bx1 - encoder batch input\n",
        "# bx2 - decoder batch input\n",
        "# by2p - encoder batch predictions\n",
        "\n",
        "def torch_beam(bx1, sos=2, eos=3, max_len=10,\n",
        "               beam_width=2, beam_depth=2, depth_reduction=torch.sum):\n",
        "  batch_size = bx1.size(0)  # input batch_size\n",
        "  bx2 = torch.full((batch_size, 2), sos, dtype=bx1.dtype)  # batch with <sos>\n",
        "  # stop when len decoder output > max_len or all decoder outputs have <eos> token\n",
        "  while bx2.size(1) - 1 < max_len and not torch.all(torch.any(bx2 == eos, axis=1)):\n",
        "    beam_scores = torch.empty((batch_size, 0))  # scores for each beam\n",
        "    for i in range(beam_depth):\n",
        "      new_batch_size = batch_size*beam_width**(i+1)\n",
        "      by2p = torch_predict(bx1, bx2)  # predict\n",
        "      next_by2p, next_bx2 = torch.topk(by2p[:,-1,:], beam_width)  # beams to top k last predictions\n",
        "      next_bx2 = next_bx2.view(new_batch_size, 1)  # new beams\n",
        "      next_by2p = next_by2p.view(new_batch_size, 1)  # new scores\n",
        "      beam_scores = torch.repeat_interleave(beam_scores, beam_width, dim=0)  # increase batch for new scores\n",
        "      bx2 = torch.repeat_interleave(bx2, beam_width, dim=0)  # increase batch for new beams\n",
        "      bx2 = torch.cat((bx2, next_bx2), 1)  # add beams\n",
        "      beam_scores = torch.cat((beam_scores, next_by2p), 1)  # add beams scores\n",
        "    beam_scores = depth_reduction(beam_scores, axis=1) # cumulative beams scores\n",
        "    beam_scores = beam_scores.view(batch_size, -1)  # split scores into batches\n",
        "    best_beams = torch.argmax(beam_scores, axis=1, keepdim=True)  # best beams\n",
        "    bx2 = bx2.view(batch_size, beam_width**beam_depth, -1)  # split beams into batches\n",
        "    # XXX its fucking magic... (return to input batch_size)\n",
        "    best_beams = best_beams.unsqueeze(2).expand(best_beams.size(0), best_beams.size(1), bx2.size(2))\n",
        "    bx2 = torch.gather(bx2, 1, best_beams)\n",
        "    bx2 = bx2.view(batch_size, -1)\n",
        "  return bx2\n",
        "\n",
        "bx1 = torch.full((5, 1), 0, dtype=torch.long)\n",
        "bx2 = torch_beam(bx1, max_len=10,\n",
        "                 beam_width=2, beam_depth=2, depth_reduction=torch.sum)\n",
        "print(bx2)\n",
        "bx2 = torch_beam(bx1, depth_reduction=torch.mean)\n",
        "print(bx2)"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2, 2, 3, 1, 3, 3, 0, 0],\n",
            "        [2, 2, 0, 0, 1, 2, 3, 1],\n",
            "        [2, 2, 0, 3, 3, 1, 2, 4],\n",
            "        [2, 2, 2, 3, 2, 4, 1, 0],\n",
            "        [2, 2, 3, 1, 3, 1, 3, 4]])\n",
            "tensor([[2, 2, 3, 1, 3, 3, 0, 0],\n",
            "        [2, 2, 0, 0, 1, 2, 3, 1],\n",
            "        [2, 2, 0, 3, 3, 1, 2, 4],\n",
            "        [2, 2, 2, 3, 2, 4, 1, 0],\n",
            "        [2, 2, 3, 1, 3, 1, 3, 4]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfgwGkcxqzZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "bt = bx1[:,1:]  # without <sos>\n",
        "print(bt)\n",
        "print(bt[bt != 0])\n",
        "bp = by2[:,:-1,:]  # without last prediction\n",
        "print(bp)\n",
        "print(bt != 0)\n",
        "print(bp[bt != 0])\n",
        "\n",
        "bp, bt = torch.tensor(bp), torch.tensor(bt)\n",
        "loss = torch.nn.NLLLoss()\n",
        "print(loss(bp[bt != 0], bt[bt != 0]))\n",
        "\n",
        "loss = torch.nn.NLLLoss(ignore_index=0)\n",
        "print(loss(torch.flatten(bp, 0, 1), torch.flatten(bt, 0)))\n",
        "\n",
        "loss = 0\n",
        "cnt = 0\n",
        "for i, t in enumerate(bt):\n",
        "  for j, l in enumerate(t):\n",
        "    print(l)\n",
        "    print(bp[i,j,l])\n",
        "    loss += bp[i,j,l].item()\n",
        "    cnt += 1\n",
        "print(loss, loss/cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}