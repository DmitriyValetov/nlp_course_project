{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ria_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPK8FBXVX0k59+0eg05R5e0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05cb230a23e64186914f80d7030f4963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fbb57173d6ce4e4da4bcb4e81dc24910",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_53bfc49798b4436e8dcc79886eb62554",
              "IPY_MODEL_fd6f7861741a44bab238b159e0927494"
            ]
          }
        },
        "fbb57173d6ce4e4da4bcb4e81dc24910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53bfc49798b4436e8dcc79886eb62554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2bc51d0944ac43d8b308dc72f661a3da",
            "_dom_classes": [],
            "description": "loading samples: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1003869,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1003869,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3f88b273afc4f298b0bc1d6411e65a4"
          }
        },
        "fd6f7861741a44bab238b159e0927494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_75580dea0ace450db8bea9f134a07c34",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1003869/1003869 [02:12&lt;00:00, 7552.12it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0693905667a4bc48f2e8dfcc125ee63"
          }
        },
        "2bc51d0944ac43d8b308dc72f661a3da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3f88b273afc4f298b0bc1d6411e65a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75580dea0ace450db8bea9f134a07c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0693905667a4bc48f2e8dfcc125ee63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a7a24cad179415e893528eed170ecd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23753775db5242b5ab0be4ad667b7910",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a4ede3983c14eaab09c0f2cbccce751",
              "IPY_MODEL_0707b9cdae9a4e7b8f8dd442a2ce8285"
            ]
          }
        },
        "23753775db5242b5ab0be4ad667b7910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a4ede3983c14eaab09c0f2cbccce751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0950a6cb8bd40659fd4a5e6c1a1de28",
            "_dom_classes": [],
            "description": "encoding samples: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1003723,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1003723,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51e899adfc1c4ca48071816f488f71fe"
          }
        },
        "0707b9cdae9a4e7b8f8dd442a2ce8285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_571eea363fcf4284ba698d0d71b476d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1003723/1003723 [02:00&lt;00:00, 8349.07it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7801c21a764748559fa8f0c19fc7f7fa"
          }
        },
        "d0950a6cb8bd40659fd4a5e6c1a1de28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51e899adfc1c4ca48071816f488f71fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "571eea363fcf4284ba698d0d71b476d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7801c21a764748559fa8f0c19fc7f7fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitriyValetov/nlp_course_project/blob/master/ria_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO83gr3P1Cqc",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzlJPNybFwgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McxTEtAaFysV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4627ab8c-73e8-490a-8081-ac5196fdde8a"
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "# 1-UtATnzLE809Vi6RLgy3GRHX2TXRzhd6  # norm\n",
        "# 1bhsdkXYEe4qixPddK9DkaQ-7z0jAn5Bi  # stop\n",
        "# 1-40QXRckYZIfJTPiAhtQ8LAeBKks8ITx  # lem\n",
        "# 1m7unmZmh0B3DJ-hiLm8-a5WoXECMc4cV  # snow\n",
        "file_id = '1-40QXRckYZIfJTPiAhtQ8LAeBKks8ITx'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "fn = downloaded.metadata['title']\n",
        "print(f'downloading: {fn}')\n",
        "downloaded.GetContentFile(fn)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading: stop_lem_norm_sents_ria.json.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfmnj01Aw_y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "fn = 'stop_lem_norm_sents_ria.json.gz'\n",
        "# fn = 'norm_sents_ria.json.gz'\n",
        "\n",
        "# first check\n",
        "with gzip.open(fn, 'rb') as f:\n",
        "  print(json.loads(next(f)))\n",
        "\n",
        "# all check\n",
        "# with gzip.open(fn, 'rb') as f:\n",
        "#   for line in tqdm(f):\n",
        "#     json.loads(line)\n",
        "\n",
        "# all check\n",
        "cnt = 0\n",
        "bad_cnt = 0\n",
        "with gzip.open(fn, 'rb') as f:\n",
        "  for line in tqdm(f):\n",
        "    cnt += 1\n",
        "    n = json.loads(line)\n",
        "    if len(n['text']) == 0 or len(n['title']) == 0:\n",
        "      bad_cnt += 1\n",
        "      print(cnt)\n",
        "      print(n)\n",
        "print(f'{bad_cnt}/{cnt}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRlGjfpXdU88",
        "colab_type": "code",
        "outputId": "d0dea9c6-bf0b-4372-b745-f6f76e3b8c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261,
          "referenced_widgets": [
            "05cb230a23e64186914f80d7030f4963",
            "fbb57173d6ce4e4da4bcb4e81dc24910",
            "53bfc49798b4436e8dcc79886eb62554",
            "fd6f7861741a44bab238b159e0927494",
            "2bc51d0944ac43d8b308dc72f661a3da",
            "d3f88b273afc4f298b0bc1d6411e65a4",
            "75580dea0ace450db8bea9f134a07c34",
            "e0693905667a4bc48f2e8dfcc125ee63",
            "5a7a24cad179415e893528eed170ecd7",
            "23753775db5242b5ab0be4ad667b7910",
            "5a4ede3983c14eaab09c0f2cbccce751",
            "0707b9cdae9a4e7b8f8dd442a2ce8285",
            "d0950a6cb8bd40659fd4a5e6c1a1de28",
            "51e899adfc1c4ca48071816f488f71fe",
            "571eea363fcf4284ba698d0d71b476d6",
            "7801c21a764748559fa8f0c19fc7f7fa"
          ]
        }
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "from pprint import pprint\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "class RiaDataset(Dataset):\n",
        "  def __init__(self, path, max_vocab=10000, max_samples=500):\n",
        "    super(RiaDataset).__init__()\n",
        "    samples = []\n",
        "    words = Counter()\n",
        "    cnt, bad_ids = 0, []\n",
        "    _, ext = os.path.splitext(path)\n",
        "    with gzip.open(path, 'rb') if ext == '.gz' else open(path) as f:\n",
        "      for _ in tqdm(range(max_samples), desc='loading samples'):\n",
        "        n = json.loads(next(f))\n",
        "        if not len(n['text']) == 0 and not len(n['title']) == 0:        \n",
        "          samples.append(n)\n",
        "          words.update(w for s in n['text'] for w in s.split())\n",
        "          words.update(w for s in n['title'] for w in s.split())\n",
        "        else:\n",
        "          bad_ids.append(cnt)\n",
        "        cnt += 1\n",
        "    print(f'bad texts: {len(bad_ids)} {bad_ids}')\n",
        "    print('making vocabulary')\n",
        "    print(f'unique words: {len(words)}, total words: {sum(words.values())}')\n",
        "    self.itos = ['<pad>', '<unk>', '<go>', '<eos>']\n",
        "    vocab_words = words.most_common(max_vocab)\n",
        "    self.itos += [w for w, _ in vocab_words]\n",
        "    self.stoi = {x: i for i, x in enumerate(self.itos)}\n",
        "    assert len(self.itos) == len(self.stoi)\n",
        "    print(f'vocabulary: {len(self.stoi)}, words in vocabulary: {sum(c for _, c in vocab_words)}')\n",
        "    print('encoding samples')\n",
        "    self.samples = []\n",
        "    for s in tqdm(samples, desc='encoding samples'):\n",
        "      x1 = [self.encode(x) for x in s['text']]\n",
        "      x2 = [self.encode(x) for x in s['title']]\n",
        "      self.samples.append([x1, x2])\n",
        "    print(f'samples: {len(self.samples)}')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.samples[i]\n",
        "\n",
        "  def encode(self, s):\n",
        "    return np.array([2] + [self.stoi.get(x, 1) for x in s.split()] + [3])\n",
        "\n",
        "  def decode(self, s):\n",
        "    return [self.itos[x] for x in s]\n",
        "\n",
        "class Collate():\n",
        "  def __init__(self, n_x1=1):\n",
        "     self.n_x1 = n_x1\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    bx1, bx2 = [], []\n",
        "    for b in batch:\n",
        "      x1s, x2s = b\n",
        "      # skip first sentence (usually date and place)\n",
        "      ei = min(len(x1s), 1 + self.n_x1)\n",
        "      si = 1 if ei > 1 else 0\n",
        "      # print(self.n_x1, si, ei, len(x1s))\n",
        "      for x1 in x1s[si:ei]:\n",
        "        for x2 in x2s:\n",
        "          bx1.append(torch.as_tensor(x1))\n",
        "          bx2.append(torch.as_tensor(x2))\n",
        "    bx1 = torch.nn.utils.rnn.pad_sequence(bx1, batch_first=True)\n",
        "    bx2 = torch.nn.utils.rnn.pad_sequence(bx2, batch_first=True)\n",
        "    batch = [bx1, bx2]\n",
        "    return batch\n",
        "\n",
        "ds = RiaDataset('stop_lem_norm_sents_ria.json.gz', max_vocab=50000, \n",
        "                max_samples=1003869)  # 1003869\n",
        "# torch.manual_seed(0)\n",
        "# torch.cuda.manual_seed(0)\n",
        "# dl = DataLoader(ds, batch_size=1, num_workers=1, shuffle=False, \n",
        "#                 drop_last=False, collate_fn=Collate())\n",
        "# for s in tqdm(dl, desc='dataset'):\n",
        "#   bx1, bx2 = s\n",
        "#   assert bx1.size()[0] == bx2.size()[0]\n",
        "#   # print(bx1.size(), bx2.size())\n",
        "#   # for x1, x2 in zip(bx1, bx2):\n",
        "#   #   print(ds.decode(x1), ds.decode(x2))\n",
        "#   # print([ds.decode(x) for x in bx1])\n",
        "#   # print([ds.decode(x) for x in bx2])\n",
        "train_len = int(0.7*len(ds))\n",
        "test_len = int(0.2*len(ds))\n",
        "val_len = len(ds) - train_len - test_len\n",
        "lens = [train_len, test_len, val_len]\n",
        "print(lens, sum(lens), len(ds))\n",
        "train_ds, test_ds, val_ds = random_split(ds, lens)\n",
        "train_dl = DataLoader(train_ds, batch_size=2, num_workers=1, \n",
        "                      shuffle=True, drop_last=False, \n",
        "                      collate_fn=Collate(3))\n",
        "test_dl = DataLoader(test_ds, batch_size=3, num_workers=1, \n",
        "                     shuffle=True, drop_last=False, \n",
        "                     collate_fn=Collate(4))\n",
        "val_dl = DataLoader(val_ds, batch_size=3, num_workers=1, \n",
        "                    shuffle=True, drop_last=False, \n",
        "                    collate_fn=Collate(2))\n",
        "# for s in tqdm(train_dl, desc='train'):\n",
        "#   bx1, bx2 = s\n",
        "#   assert bx1.size()[0] == bx2.size()[0]\n",
        "#   # print(bx1.size(), bx2.size())\n",
        "# for s in tqdm(test_dl, desc='test'):\n",
        "#   bx1, bx2 = s\n",
        "#   assert bx1.size()[0] == bx2.size()[0]\n",
        "#   # print(bx1.size(), bx2.size())\n",
        "# for s in tqdm(val_dl, desc='val'):\n",
        "#   bx1, bx2 = s\n",
        "#   assert bx1.size()[0] == bx2.size()[0]\n",
        "#   # print(bx1.size(), bx2.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05cb230a23e64186914f80d7030f4963",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='loading samples', max=1003869.0, style=ProgressStyle(desc…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bad texts: 146 [85124, 121073, 167715, 176024, 191679, 196491, 233761, 240737, 248487, 260013, 285129, 292992, 293089, 293116, 301604, 305587, 312586, 314213, 316134, 317722, 318000, 318655, 325713, 362998, 366397, 382262, 397968, 400780, 406416, 411736, 431649, 437910, 453488, 463202, 463257, 467341, 471837, 477691, 480756, 483624, 487196, 493816, 495050, 495318, 495338, 495705, 495898, 498775, 499475, 501329, 503532, 503880, 504118, 504486, 504600, 508333, 510257, 517735, 518394, 519404, 519432, 519506, 520672, 521438, 521730, 524884, 525492, 526160, 526173, 527055, 527102, 527562, 529467, 529469, 530575, 531190, 532894, 533960, 533982, 537247, 540254, 540565, 542038, 542044, 542769, 544866, 545213, 550244, 552215, 557118, 570332, 571466, 572783, 572920, 573314, 574084, 575678, 575691, 583560, 612454, 666619, 680547, 681071, 690771, 693126, 778882, 781772, 794032, 795809, 804124, 819613, 822100, 858471, 868287, 906565, 908647, 909081, 909141, 911907, 912070, 914943, 915590, 916612, 922892, 923407, 923424, 923897, 925689, 929368, 929439, 929926, 932485, 932907, 932965, 934383, 934394, 935243, 942362, 948453, 957861, 966437, 966458, 966462, 966470, 982507, 983568]\n",
            "making vocabulary\n",
            "unique words: 601956, total words: 198199252\n",
            "vocabulary: 50004, words in vocabulary: 194385367\n",
            "encoding samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a7a24cad179415e893528eed170ecd7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='encoding samples', max=1003723.0, style=ProgressStyle(des…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "samples: 1003723\n",
            "[702606, 200744, 100373] 1003723 1003723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VquclFh0W_m7",
        "colab_type": "code",
        "outputId": "7d0d39d4-f3ea-4afe-d393-19269c1e9c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3011173"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwYBLWZRXQqS",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfI1NnKX9c1o",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, num_embeddings, embedding_dim, padding_idx,\n",
        "               rnn_type, hidden_size, num_layers=1, rnn_dropout=0,\n",
        "               bidirectional=False, dropout=0, pack=False):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    rnn_map = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}\n",
        "    self.embedding = nn.Embedding(num_embeddings=num_embeddings, \n",
        "                                  embedding_dim=embedding_dim,\n",
        "                                  padding_idx=padding_idx)\n",
        "    self.rnn = rnn_map[rnn_type](input_size=embedding_dim, \n",
        "                                 hidden_size=hidden_size,\n",
        "                                 num_layers=num_layers,\n",
        "                                 batch_first=True,\n",
        "                                 dropout=rnn_dropout,\n",
        "                                 bidirectional=bidirectional)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.pack = pack\n",
        "\n",
        "  def forward(self, x1):  # [B, L]\n",
        "    x = self.embedding(x1)  # [B, L] -> [B, L, E]\n",
        "    if self.pack:  # [B, L, E] -> Packed [B, L, E]\n",
        "      ls = torch.sum(x1 != 0, dim=1)\n",
        "      x = pack_padded_sequence(x, ls, batch_first=True, enforce_sorted=False)\n",
        "    # [B, L, E] -> (Packed) [B, L, ND*H], ([NL*ND, B, H], ([NL*ND, B, H]))\n",
        "    if isinstance(self.rnn, nn.LSTM):\n",
        "      ht, (hn, cn) = self.rnn(x)\n",
        "      hn, cn = self.dropout(hn), self.dropout(cn)\n",
        "      return ht, (hn, cn)\n",
        "    else:\n",
        "      ht, hn = self.rnn(x)\n",
        "      hn = self.dropout(hn)\n",
        "      return ht, hn\n",
        "\n",
        "\n",
        "class AttentionDecoderRNN(nn.Module):\n",
        "  def __init__(self, num_embeddings, embedding_dim, padding_idx,\n",
        "               rnn_type, hidden_size, num_layers=1, rnn_dropout=0,\n",
        "               bidirectional=False, dropout=0, out_hidden=0,\n",
        "               attn_type='soft_dot', pack=False):\n",
        "    super(AttentionDecoderRNN, self).__init__()\n",
        "    rnn_map = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}\n",
        "    attn_types = ['dot', 'cos', 'dist', 'soft_dot', 'soft_cos', 'soft_dist', 'none']\n",
        "    self.attn_type = attn_type\n",
        "    self.embedding = nn.Embedding(num_embeddings=num_embeddings, \n",
        "                                  embedding_dim=embedding_dim,\n",
        "                                  padding_idx=padding_idx)\n",
        "    self.rnn = rnn_map[rnn_type](input_size=embedding_dim, \n",
        "                                 hidden_size=hidden_size,\n",
        "                                 num_layers=num_layers,\n",
        "                                 batch_first=True,\n",
        "                                 dropout=rnn_dropout,\n",
        "                                 bidirectional=bidirectional)\n",
        "    out_input = hidden_size if self.attn_type == 'none' else 2*hidden_size\n",
        "    if out_hidden > 0:\n",
        "      self.out_hidden = nn.Linear(out_input, out_hidden)\n",
        "      self.out = nn.Linear(out_hidden, num_embeddings)\n",
        "    else:\n",
        "      self.out_hidden = None\n",
        "      self.out = nn.Linear(out_input, num_embeddings)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.softmax = nn.LogSoftmax(dim=2)\n",
        "    self.pack = pack\n",
        "\n",
        "  def forward(self, x2, h1, x1):  # [B, L], [B, L, ND*H], ([NL*ND, B, H], [NL*ND, B, H])\n",
        "    x = self.embedding(x2)  # [B, L] -> [B, L, E]\n",
        "    if self.pack:  # [B, L, E] -> Packed [B, L, E]\n",
        "      lengths = torch.sum(x2 != 0, dim=1)\n",
        "      x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "    ht1, hns1 = h1  # [B, L, ND*H], ([NL*ND, B, H], [NL*ND, B, H])\n",
        "    # [B, L, D], ([NL*ND, B, H], [NL*ND, B, H]) -> [B, L, ND*H], ([NL*ND, B, H], [NL*ND, B, H])\n",
        "    ht2, hns2 = self.rnn(x, hns1)\n",
        "    if self.pack:  # Packed [B, L, E] -> [B, L, E]\n",
        "      ht1, _ = pad_packed_sequence(ht1, batch_first=True)\n",
        "      ht2, _ = pad_packed_sequence(ht2, batch_first=True)\n",
        "    if self.attn_type == 'none':\n",
        "      if self.rnn.bidirectional:\n",
        "        B2, L2, NDH2 = ht2.size()\n",
        "        # [B, L, ND*H] -> [B, L, ND, H] -> [B, L, H]\n",
        "        ht2 = ht2.view(B2, L2, 2, int(NDH2/2)).mean(2)\n",
        "      x = self.dropout(ht2)\n",
        "      if self.out_hidden is not None:\n",
        "        x = self.out_hidden(x)\n",
        "      x = self.out(x)\n",
        "      y2 = self.softmax(x)\n",
        "      return y2, None\n",
        "    else:\n",
        "      if self.rnn.bidirectional:\n",
        "        B1, L1, NDH1 = ht1.size()\n",
        "        B2, L2, NDH2 = ht2.size()  # B1 == B2, NDH1 == NDH2\n",
        "        # [B, L, ND*H] -> [B, L, ND, H] -> [B, L, H]\n",
        "        ht1 = ht1.view(B1, L1, 2, int(NDH1/2)).mean(2)\n",
        "        ht2 = ht2.view(B2, L2, 2, int(NDH2/2)).mean(2)\n",
        "      # [L2, H], [L1, H] -> [L2, L1]\n",
        "      # mask where x1 and x2 token is <PAD>\n",
        "      # pad_mask = torch.einsum('bi,bj->bij', x2, x1) == 0\n",
        "      # mask where only x1 token is <PAD>\n",
        "      pad_mask = torch.einsum('bi,bj->bij', torch.ones_like(x2), x1) == 0\n",
        "      # [B, L2, H], [B, L1, H] -> [B, L2, L1]  # attention\n",
        "      # [B, L2, L1], [B, L1, H] -> [B, L2, H]  # weighted h1\n",
        "      if self.attn_type == 'dot':\n",
        "        attn = torch.einsum('bih,bjh->bij', ht2, ht1)  # dot product\n",
        "        attn[pad_mask] = 0\n",
        "      elif self.attn_type == 'soft_dot':\n",
        "        attn = torch.einsum('bih,bjh->bij', ht2, ht1)  # dot product\n",
        "        attn[pad_mask] = float('-inf')\n",
        "        attn = F.softmax(attn, 2)\n",
        "      elif self.attn_type == 'dist':\n",
        "        ht1, ht2 = ht1.contiguous(), ht2.contiguous()\n",
        "        attn = torch.cdist(ht2, ht1)  # euclidian distance\n",
        "        attn = torch.masked_fill(attn, pad_mask, float('inf'))\n",
        "        attn = F.threshold(attn, threshold=1e-6, value=1e-6)  # short dist\n",
        "        attn = 1/attn  # inverse\n",
        "        attn = F.normalize(attn, p=1, dim=2)  # to [0, 1]\n",
        "      elif self.attn_type == 'soft_dist':\n",
        "        ht1, ht2 = ht1.contiguous(), ht2.contiguous()\n",
        "        attn = torch.cdist(ht2, ht1)  # euclidian distance\n",
        "        attn = torch.masked_fill(attn, pad_mask, float('inf'))\n",
        "        attn = F.softmin(attn, 2)\n",
        "      elif self.attn_type == 'cos':\n",
        "        ht1n = F.normalize(ht1, p=2, dim=2)  # normalize to length = 1\n",
        "        ht2n = F.normalize(ht2, p=2, dim=2)  # normalize to length = 1\n",
        "        attn = torch.einsum('bih,bjh->bij', ht2n, ht1n)  # dot product\n",
        "        attn[pad_mask] = 0\n",
        "      elif self.attn_type == 'soft_cos':\n",
        "        ht1n = F.normalize(ht1, p=2, dim=2)  # normalize to length = 1\n",
        "        ht2n = F.normalize(ht2, p=2, dim=2)  # normalize to length = 1\n",
        "        attn = torch.einsum('bih,bjh->bij', ht2n, ht1n)  # dot product\n",
        "        attn[pad_mask] = float('-inf')\n",
        "        attn = F.softmax(attn, 2)\n",
        "      hw1 = torch.einsum('bij,bjh->bih', attn, ht1)  # weighted h1\n",
        "      ha = torch.cat((ht2, hw1), 2)  # [B, L2, H], [B, L2, H] -> [B, L2, H+H]\n",
        "      x = self.dropout(ha)  # [B, L2, H+H] -> [B, L2, H+H]\n",
        "      if self.out_hidden is not None:\n",
        "        x = self.out_hidden(x)   # [B, L2, H+H] -> [B, L2, OH]\n",
        "      x = self.out(x)  # [B, L2, H+H] or [B, L2, OH] -> [B, L2, D2]\n",
        "      y2 = self.softmax(x)  # [B, L2, D2] -> [B, L2, D2]\n",
        "      return y2, attn\n",
        "\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "  def __init__(self, \n",
        "               enc_num_embeddings, enc_embedding_dim, enc_padding_idx,\n",
        "               dec_num_embeddings, dec_embedding_dim, dec_padding_idx,\n",
        "               rnn_type, hidden_size, num_layers=1, out_hidden=0,\n",
        "               enc_rnn_dropout=0, dec_rnn_dropout=0,\n",
        "               bidirectional=False, enc_dropout=0, dec_dropout=0, \n",
        "               attn_type='dot', pack=False):\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    self.encoder = EncoderRNN(num_embeddings=enc_num_embeddings, \n",
        "                              embedding_dim=enc_embedding_dim, \n",
        "                              padding_idx=enc_padding_idx,\n",
        "                              hidden_size=hidden_size, \n",
        "                              rnn_type=rnn_type, \n",
        "                              bidirectional=bidirectional,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout=enc_dropout,\n",
        "                              rnn_dropout=enc_rnn_dropout,\n",
        "                              pack=pack)\n",
        "    self.decoder = AttentionDecoderRNN(num_embeddings=dec_num_embeddings, \n",
        "                                       embedding_dim=dec_embedding_dim, \n",
        "                                       padding_idx=dec_padding_idx,\n",
        "                                       hidden_size=hidden_size, \n",
        "                                       rnn_type=rnn_type, \n",
        "                                       bidirectional=bidirectional,\n",
        "                                       num_layers=num_layers,\n",
        "                                       dropout=dec_dropout,\n",
        "                                       out_hidden=out_hidden,\n",
        "                                       rnn_dropout=dec_rnn_dropout,\n",
        "                                       attn_type=attn_type,\n",
        "                                       pack=pack)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    #  [B, L1] -> [B, L1, ND*H], ([NL*ND, B, H], [NL*ND, B, H])\n",
        "    h1 = self.encoder(x1)\n",
        "    # [B, L2], ([B, L1, ND*H], ([NL*ND, B, H], [NL*ND, B, H])) -> [B, L2, E2]\n",
        "    y2 = self.decoder(x2, h1, x1)\n",
        "    return y2\n",
        "\n",
        "\n",
        "def external_attn(x1, x2, attn_dict):\n",
        "    attn_dict = {1: [1], 2: [2], 4: [4, 3]}\n",
        "    attn = []\n",
        "    B1, L1 = x1.size()\n",
        "    B2, L2 = x2.size()\n",
        "    for i in range(B1):\n",
        "      b = []\n",
        "      for j in range(L2):\n",
        "        l2 = []\n",
        "        # x2t = x2[i, j].item()\n",
        "        x2t = x2[i, j + 1].item() if j + 2 < L2 else 0 # decoder shift\n",
        "        for k in range(L1):\n",
        "          x1t = x1[i, k].item()\n",
        "          # x1t = x1[i, k + 1].item() if k + 1 < L1 else 0 # encoder shift\n",
        "          x1ts = self.attn_dict.get(x2t, [])\n",
        "          if x1t in x1ts:\n",
        "            l2.append(1.)\n",
        "          else:\n",
        "            l2.append(0.)\n",
        "        b.append(l2)\n",
        "      attn.append(b)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    attn = torch.tensor(attn).to(device)\n",
        "    attn = F.normalize(attn, p=1, dim=2)  # to [0, 1]\n",
        "    return attn\n",
        "\n",
        "\n",
        "from IPython.display import Image\n",
        "# Image(make_dot(loss).render('loss', format='png'))\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "def plot_attention(a, x1, x2, shift=True, mask=True, \n",
        "                   suptitle='Attention', figsize=None, \n",
        "                   decoder_x1=None, decoder_x2=None, tight=False,\n",
        "                   labels=True):\n",
        "  # %matplotlib inline\n",
        "  import matplotlib.pyplot as plt\n",
        "  from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "  b = a.shape[0]\n",
        "  fig, axs = plt.subplots(1, b, figsize=figsize)\n",
        "  if not isinstance(axs, np.ndarray):  # if batch size == 1\n",
        "    axs = [axs]\n",
        "  fig.suptitle(suptitle)\n",
        "  for i in range(b):\n",
        "    ax = axs[i]\n",
        "    if shift:\n",
        "      ba, bx1, bx2 = a[i,:-1,1:], x1[i,1:], x2[i,1:]\n",
        "    else:\n",
        "      ba, bx1, bx2 = a[i], x1[i], x2[i]\n",
        "    if mask:\n",
        "      mask_x1 = np.flatnonzero(bx1)\n",
        "      mask_x2 = np.flatnonzero(bx2)\n",
        "      ba, bx1, bx2 =  ba[mask_x2,:][:,mask_x1], bx1[mask_x1], bx2[mask_x2]\n",
        "    # ax.set_title(f'{i+1}', y=-0.2)\n",
        "    im = ax.imshow(ba, cmap='gray')\n",
        "    ax.set_xticks(np.arange(len(bx1)))\n",
        "    ax.set_yticks(np.arange(len(bx2)))\n",
        "    if labels:\n",
        "      ax.set_xlabel('x1')\n",
        "      ax.set_ylabel('x2')\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    if decoder_x1 is not None:\n",
        "      bx1 = decoder_x1(bx1)\n",
        "    if decoder_x2 is not None:\n",
        "      bx2 = decoder_x2(bx2)\n",
        "    ax.set_xticklabels(bx1, rotation=90)  # rotation=90\n",
        "    ax.set_yticklabels(bx2)\n",
        "    fig.colorbar(im, ax=ax, fraction=0.05, pad=0.05)\n",
        "  if tight:\n",
        "    plt.tight_layout()\n",
        "\n",
        "# torch.backends.cudnn.enabled=False\n",
        "# torch.backends.cudnn.deterministic=True\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# enc_dec_config = {\n",
        "#   'enc_num_embeddings': 6,\n",
        "#   'enc_embedding_dim': 2,\n",
        "#   'enc_padding_idx': 0,\n",
        "#   'dec_num_embeddings': 6,\n",
        "#   'dec_embedding_dim': 2,\n",
        "#   'dec_padding_idx': 0,\n",
        "#   'rnn_type': 'RNN',\n",
        "#   'hidden_size': 2,\n",
        "#   'num_layers': 1,\n",
        "#   'bidirectional': False,\n",
        "#   'enc_rnn_dropout': 0,\n",
        "#   'dec_rnn_dropout': 0,\n",
        "#   'enc_dropout': 0,\n",
        "#   'dec_dropout': 0,\n",
        "#   'attn_type': 'soft_dist',\n",
        "#   'out_hidden': 16,\n",
        "#   'pack': True\n",
        "# }\n",
        "# # <pad>, <unk>, <go>, <eos>\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# seed = 0\n",
        "# torch.manual_seed(seed)\n",
        "# torch.cuda.manual_seed(seed)\n",
        "# model = EncoderDecoder(**enc_dec_config).to(device)\n",
        "# x1 = torch.tensor([[2, 1, 4, 3, 0], [2, 1, 4, 1, 3]]).to(device)\n",
        "# x2 = torch.tensor([[2, 4, 1, 5, 3, 0], [2, 4, 5, 1, 1, 3]]).to(device)\n",
        "# opt = optim.Rprop(model.parameters(), lr=0.01)\n",
        "# loss_fn = torch.nn.NLLLoss(ignore_index=0)  # 0 is <PAD>\n",
        "# pbar = tqdm(range(200))\n",
        "# for i in pbar:\n",
        "#   opt.zero_grad()\n",
        "#   t = x2[:,1:].flatten(start_dim=0)\n",
        "#   y2, attn = model(x1, x2)\n",
        "#   p = y2[:,:-1,:].flatten(start_dim=0, end_dim=1)\n",
        "#   loss = loss_fn(p, t)\n",
        "#   loss.backward()\n",
        "#   opt.step()\n",
        "#   with torch.no_grad():\n",
        "#     idx = torch.nonzero(t).view(-1)\n",
        "#     acc = torch.sum(torch.argmax(p, 1)[idx] == t[idx]).float()/idx.size()[0]\n",
        "#     ps = torch.argmax(y2[:,:-1,:], 2).detach().cpu().numpy()\n",
        "#     ts = x2[:,1:].detach().cpu().numpy()\n",
        "#     for sp, st in zip(ps, ts):\n",
        "#       t_set = set(st) - {0, 1, 2, 3}\n",
        "#       eos = sp[sp == 3][0] if 3 in sp else len(sp)\n",
        "#       p_eos = sp[:eos]\n",
        "#       p_set = set(p_eos) - {0, 1, 2, 3}\n",
        "#       i_set = t_set.intersection(p_set)\n",
        "#       pre = len(i_set)/len(p_set) if len(p_set) != 0 else 0\n",
        "#       rec = len(i_set)/len(t_set) if len(t_set) != 0 else 0\n",
        "#       f1 = 2*pre*rec/(pre + rec) if pre + rec != 0 else 0\n",
        "#       # print(st, sp, p_eos)\n",
        "#       # print(t_set, p_set)\n",
        "#       # print(f'precision: {pre}, recall: {rec}, f1: {f1}')\n",
        "#     if i % 25 == 0:\n",
        "#       # print(attn)\n",
        "#       # print(p)\n",
        "#       # print(torch.argmax(p, 1))\n",
        "#       # print(t)\n",
        "#       if attn is not None:\n",
        "#          plot_attention(attn.detach().cpu().numpy(), \n",
        "#                         x1.detach().cpu().numpy(), \n",
        "#                         x2.detach().cpu().numpy(),\n",
        "#                         labels=True, tight=False,\n",
        "#                         shift=False, mask=False, figsize=(10, 5))\n",
        "#   pbar.set_description(f'loss: {loss:.3f}, acc: {acc:.3f}')\n",
        "#   if acc == 1:\n",
        "#     break\n",
        "# # Image(make_dot(attn).render('attn', format='png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZPHNG9iXXou",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSxvMYhCpQLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install optuna\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icz-Yt2dikV-",
        "colab_type": "code",
        "outputId": "1e518b35-5a19-4d50-a89e-fb5e5e4f8b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import subprocess\n",
        "print(subprocess.getstatusoutput('nvidia-smi')[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May 15 23:34:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    31W / 250W |    715MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUcqiqInpBGs",
        "colab_type": "code",
        "outputId": "a2264105-5bdf-4019-e928-1e6a5ea34f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb4ephjLpFFY",
        "colab_type": "code",
        "outputId": "3f0794e3-d09b-4daf-807f-212ecf994074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Import packages\n",
        "import os, sys, humanize, psutil, GPUtil\n",
        "\n",
        "# Define function\n",
        "def mem_report():\n",
        "  print(\"CPU RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available ))\n",
        "  \n",
        "  GPUs = GPUtil.getGPUs()\n",
        "  for i, gpu in enumerate(GPUs):\n",
        "    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "    \n",
        "# Execute function\n",
        "mem_report()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU RAM Free: 19.5 GB\n",
            "GPU 0 ... Mem Free: 15565MB / 16280MB | Utilization   4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OElaFmqpOwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import optuna\n",
        "from tqdm.notebook import tqdm\n",
        "from pprint import pprint\n",
        "import random\n",
        "import numpy as np\n",
        "from shutil import copyfile\n",
        "\n",
        "def run(run_type, device, model, opt, loss_fn, dl, ds, print_triples=False, \n",
        "        plot_attn=False, inference='forced'):\n",
        "  losses, accs, pres, recs, f1s = [], [], [], [], []\n",
        "  pbar = tqdm(dl)\n",
        "  if run_type == 'train':\n",
        "    model.train()\n",
        "    for x1, x2 in pbar:\n",
        "      opt.zero_grad()\n",
        "      x1 = x1.to(device)\n",
        "      x2 = x2.to(device)\n",
        "      t = x2[:,1:].flatten(0)\n",
        "      y2, attn = model(x1, x2)\n",
        "      p = y2[:,:-1,:].flatten(0, 1)\n",
        "      loss = loss_fn(p, t)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      losses.append(loss.item())\n",
        "      idx = torch.nonzero(t).view(-1)\n",
        "      acc = torch.sum(torch.argmax(p, 1)[idx] == t[idx]).float()/idx.size()[0]\n",
        "      accs.append(acc.item())\n",
        "      ps = torch.argmax(y2[:,:-1,:], 2).detach().cpu().numpy()\n",
        "      ts = x2[:,1:].detach().cpu().numpy()\n",
        "      for sp, st in zip(ps, ts):\n",
        "        t_set = set(st) - {0, 1, 2, 3}\n",
        "        eos = sp[sp == 3][0] if 3 in sp else len(sp)\n",
        "        p_eos = sp[:eos]\n",
        "        p_set = set(p_eos) - {0, 1, 2, 3}\n",
        "        i_set = t_set.intersection(p_set)\n",
        "        pre = len(i_set)/len(p_set) if len(p_set) != 0 else 0\n",
        "        rec = len(i_set)/len(t_set) if len(t_set) != 0 else 0\n",
        "        f1 = 2*pre*rec/(pre + rec) if pre + rec != 0 else 0\n",
        "        # print(st, sp, p_eos)\n",
        "        # print(t_set, p_set)\n",
        "        # print(f'precision: {pre}, recall: {rec}, f1: {f1}')\n",
        "        pres.append(pre)\n",
        "        recs.append(rec)\n",
        "        f1s.append(f1)\n",
        "      pbar.set_description(f'{run_type} l: {loss.item():.2f}, a: {acc:.2f}, p: {pre:.2f}, r: {rec:.2f}, f1: {f1:.2f}')\n",
        "  else:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x1, x2 in pbar:\n",
        "        x1 = x1.to(device)\n",
        "        x2 = x2.to(device)\n",
        "        t = x2[:,1:].flatten(0)\n",
        "        if inference == 'forced':\n",
        "          y2, attn = model(x1, x2)\n",
        "        elif inference == 'greedy':\n",
        "          # max_len = 30\n",
        "          cx2 = x2[:,:1]\n",
        "          for i in range(x2.size()[1]):\n",
        "            y2, attn = model(x1, cx2)\n",
        "            cy2 = y2[:,i,:]\n",
        "            nx2 = torch.argmax(cy2, 1)\n",
        "            nx2 = nx2.unsqueeze(1)\n",
        "            cx2 = torch.cat((cx2, nx2), 1)\n",
        "        elif inference == 'beam':\n",
        "          i = 0\n",
        "          cx2 = x2[:,:1]\n",
        "          # print(f'cx2 {cx2.size()}')\n",
        "          y2, attn = model(x1, cx2)\n",
        "          cy2 = y2[:,i,:]\n",
        "          nx2 = torch.argmax(cy2, 1)\n",
        "          nx2 = nx2.unsqueeze(1)\n",
        "          emb_size = y2.size()[2]\n",
        "          batch_size = y2.size()[0]\n",
        "          # print(emb_size)\n",
        "          beam_size = 2\n",
        "          beam_depth = 4\n",
        "          print('before')\n",
        "          print(cy2[:,0])\n",
        "          print(torch.max(cy2, 1))\n",
        "          top = torch.topk(cy2, beam_size)\n",
        "          bx2 = top[1].view(beam_size*batch_size, -1)\n",
        "          print(bx2)\n",
        "          cxx2 = torch.cat((cx2.repeat(beam_size, 1), nx2.repeat(beam_size, 1), bx2), 1)\n",
        "          for j in range(beam_depth):\n",
        "            # print(cx2.expand(-1, beam_size).view(beam_size*batch_size, -1))\n",
        "            # print(cx2.)\n",
        "            # print(cx2.repeat(beam_size, 1))\n",
        "            print(cxx2)\n",
        "            yy2, attn = model(x1.repeat(beam_size, 1), cxx2)\n",
        "            # print(yy2)\n",
        "            cyy2 = yy2[:,-1,:]\n",
        "            nxx2 = torch.argmax(cyy2, 1)\n",
        "            nxx2 = nxx2.unsqueeze(1)\n",
        "            cxx2 = torch.cat((cxx2, nxx2), 1)\n",
        "          # for j in range(1, emb_size): # pad problem\n",
        "          #   beam = torch.full([batch_size, 1], j, dtype=int).to(device)\n",
        "          #   cxx2 = torch.cat((cx2, nx2, beam), 1)\n",
        "          #   # print(cxx2)\n",
        "          #   # print(f'cxx2 {cxx2.size()}')\n",
        "          #   yy2, attn = model(x1, cxx2)\n",
        "          #   cyy2 = yy2[:,i+1,:]  # B, L, E -> B, E\n",
        "          #   # Sum - prev_y = argmax(prev_y + cur_y)\n",
        "          #   # print(cyy2.size(), cy2[:,j].size())\n",
        "          #   # print(cy2)\n",
        "          #   # print(cyy2[0])\n",
        "          #   # print(cy2[0:,j])\n",
        "          #   # B, E + B -> B, E + B, 1 -broadcasting-> B, E + B, E\n",
        "          #   summ = torch.add(cyy2, cy2[:,j].unsqueeze(1))\n",
        "          #   # print(summ[0])\n",
        "          #   # print(summ.size())\n",
        "          #   # print('max')\n",
        "          #   max_summ = torch.max(summ, 1)[0]\n",
        "          #   # print(max_summ)\n",
        "          #   # print('before')\n",
        "          #   # print(cy2[:,j])\n",
        "          #   cy2[:,j] = max_summ\n",
        "          #   # print('after')\n",
        "          #   # print(cy2[:,j])\n",
        "          #   # print(max_summ.size())\n",
        "          #   # Viterbi - prev_y = argmax(cur_y)\n",
        "          #   # nxx2 = torch.argmax(cy2, 1)\n",
        "          #   # nx2 = nx2.unsqueeze(1)\n",
        "          #   # cx2 = torch.cat((cx2, nx2), 1)\n",
        "          print('after')\n",
        "          print(cy2[:,0])\n",
        "          print(torch.max(cy2, 1))\n",
        "        p = y2[:,:-1,:].flatten(0, 1)\n",
        "        loss = loss_fn(p, t)\n",
        "        losses.append(loss.item())\n",
        "        idx = torch.nonzero(t).view(-1)\n",
        "        acc = torch.sum(torch.argmax(p, 1)[idx] == t[idx]).float()/idx.size()[0]\n",
        "        accs.append(acc.item())\n",
        "        ps = torch.argmax(y2[:,:-1,:], 2).detach().cpu().numpy()\n",
        "        ts = x2[:,1:].detach().cpu().numpy()\n",
        "        for sp, st in zip(ps, ts):\n",
        "          t_set = set(st) - {0, 1, 2, 3}\n",
        "          eos = sp[sp == 3][0] if 3 in sp else len(sp)\n",
        "          p_eos = sp[:eos]\n",
        "          p_set = set(p_eos) - {0, 1, 2, 3}\n",
        "          i_set = t_set.intersection(p_set)\n",
        "          pre = len(i_set)/len(p_set) if len(p_set) != 0 else 0\n",
        "          rec = len(i_set)/len(t_set) if len(t_set) != 0 else 0\n",
        "          f1 = 2*pre*rec/(pre + rec) if pre + rec != 0 else 0\n",
        "          pres.append(pre)\n",
        "          recs.append(rec)\n",
        "          f1s.append(f1)\n",
        "        pbar.set_description(f'{run_type} l: {loss.item():.2f}, a: {acc:.2f}, p: {pre:.2f}, r: {rec:.2f}, f1: {f1:.2f}')\n",
        "  m_loss = sum(losses)/len(losses)\n",
        "  m_acc = sum(accs)/len(accs)\n",
        "  m_pre = sum(pres)/len(pres)\n",
        "  m_rec = sum(recs)/len(recs)\n",
        "  m_f1 = sum(f1s)/len(f1s)\n",
        "  if print_triples:\n",
        "    dy2 = map(ds.decode, torch.argmax(y2[:,:-1,:], 2).detach().cpu().numpy())\n",
        "    dx2 = map(ds.decode, x2[:,1:].detach().cpu().numpy())\n",
        "    dx1 = map(ds.decode, x1[:,1:].detach().cpu().numpy())\n",
        "    triples = list(zip(dx1, dx2, dy2))[:1]\n",
        "    for triple in triples:\n",
        "      print(triple[0])\n",
        "      print(triple[1])\n",
        "      print(triple[2])\n",
        "  if plot_attn:\n",
        "    plot_attention(attn.detach().cpu().numpy()[:1], \n",
        "                   x1.detach().cpu().numpy()[:1], \n",
        "                   x2.detach().cpu().numpy()[:1],\n",
        "                   decoder_x1 = ds.decode,\n",
        "                   decoder_x2 = ds.decode,\n",
        "                   figsize=(10, 10),\n",
        "                   shift=True,\n",
        "                   mask=True,\n",
        "                   suptitle=None)\n",
        "  return m_loss, m_acc, m_pre, m_rec, m_f1\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  mem_report()\n",
        "  torch.cuda.empty_cache()\n",
        "  mem_report()\n",
        "  trial.set_user_attr('device', str(device))\n",
        "  opt_fn_map = {\n",
        "    'SGD': optim.SGD,\n",
        "    'Adam': optim.Adam,\n",
        "    'Adagrad': optim.Adagrad,\n",
        "    'ASGD': optim.ASGD,\n",
        "    'Adamax': optim.Adamax,\n",
        "    'SparseAdam': optim.SparseAdam,\n",
        "    'AdamW': optim.AdamW,\n",
        "    'Adadelta': optim.Adadelta,\n",
        "    'LBFGS': optim.LBFGS,\n",
        "    'RMSprop': optim.RMSprop,\n",
        "    'Rprop': optim.Rprop\n",
        "  }\n",
        "  checkpoint = None\n",
        "  # fn = 'model.pth'\n",
        "  fn = f'model_{trial.study.study_name}_{trial.number}.pth'\n",
        "  print(fn)\n",
        "  # checkpoint = torch.load('model.pth')\n",
        "  if checkpoint is None:\n",
        "    train_config = {\n",
        "      'seed': trial.suggest_int('seed', 0, 0),\n",
        "      'n_epoches': trial.suggest_int('n_epoches', 5, 5),\n",
        "      'lr': trial.suggest_loguniform('lr', 1e-4, 1e-2),\n",
        "      # 'opt_fn': trial.suggest_categorical('opt_fn', ['SGD', 'Adam', 'Adagrad', \n",
        "      #                                           'ASGD', 'Adamax',\n",
        "      #                                           'AdamW', 'Adadelta',\n",
        "      #                                           'RMSprop', 'Rprop']),\n",
        "      'opt_fn': trial.suggest_categorical('opt_fn', ['Adam']),\n",
        "      'batch_size': trial.suggest_int('batch_size', 300, 300),\n",
        "      'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-6),\n",
        "      'nx1': trial.suggest_int('nx1', 1, 2)\n",
        "    }\n",
        "    model_config = {\n",
        "      'enc_num_embeddings': len(ds.stoi),\n",
        "      'enc_embedding_dim': trial.suggest_int('enc_embedding_dim', 300, 300),\n",
        "      'enc_padding_idx': 0,\n",
        "      'dec_num_embeddings': len(ds.stoi),\n",
        "      'dec_embedding_dim': trial.suggest_int('dec_embedding_dim', 300, 300),\n",
        "      'dec_padding_idx': 0,\n",
        "      'rnn_type': trial.suggest_categorical('rnn_type', ['RNN']),\n",
        "      # 'rnn_type': trial.suggest_categorical('rnn_type', ['RNN']),\n",
        "      'hidden_size': trial.suggest_int('hidden_size', 300, 300),\n",
        "      'num_layers': trial.suggest_int('num_layers', 1, 1),\n",
        "      'bidirectional': trial.suggest_categorical('bidirectional', [False]),\n",
        "      # 'bidirectional': trial.suggest_categorical('bidirectional', [True]),\n",
        "      'enc_dropout': trial.suggest_uniform('enc_dropout', 0.0, 0.0),\n",
        "      'dec_dropout': trial.suggest_uniform('dec_dropout', 0.0, 0.0),\n",
        "      'enc_rnn_dropout': trial.suggest_uniform('enc_rnn_dropout', 0.0, 0.0),\n",
        "      'dec_rnn_dropout': trial.suggest_uniform('dec_rnn_dropout', 0.0, 0.0),\n",
        "      'attn_type': trial.suggest_categorical('attn_type', ['soft_dist']),\n",
        "      # 'attn_type': trial.suggest_categorical('attn_type', ['dot', 'dist', 'soft_dot', 'soft_dist', 'cos', 'soft_cos', 'none']),\n",
        "      'out_hidden': trial.suggest_int('out_hidden', 600, 600),\n",
        "      'pack': trial.suggest_categorical('pack', [True]),\n",
        "    }\n",
        "  else:\n",
        "    model_config = checkpoint['model_config']\n",
        "    train_config = checkpoint['train_config']\n",
        "  # train\n",
        "  seed = train_config['seed']\n",
        "  opt_fn = train_config['opt_fn']\n",
        "  lr = train_config['lr']\n",
        "  n_epoches = train_config['n_epoches']\n",
        "  batch_size = train_config['batch_size']\n",
        "  weight_decay = train_config['weight_decay']\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  mem_report()\n",
        "  model = EncoderDecoder(**model_config).to(device)\n",
        "  if checkpoint is not None:\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  mem_report()\n",
        "  if opt_fn not in ['Rprop']:\n",
        "    opt = opt_fn_map[opt_fn](model.parameters(), lr=lr, \n",
        "                             weight_decay=weight_decay)\n",
        "  else:\n",
        "    opt = opt_fn_map[opt_fn](model.parameters(), lr=lr)\n",
        "  loss_fn = torch.nn.NLLLoss(ignore_index=0)\n",
        "  trial.set_user_attr('n_samples', len(ds))\n",
        "  train_len = int(0.7*len(ds))\n",
        "  test_len = int(0.2*len(ds))\n",
        "  val_len = len(ds) - train_len - test_len\n",
        "  lens = [train_len, test_len, val_len]\n",
        "  # print(lens)\n",
        "  train_ds, test_ds, val_ds = random_split(ds, lens)\n",
        "  train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=1, \n",
        "                        shuffle=True, drop_last=False,\n",
        "                        collate_fn=Collate(train_config['nx1']))\n",
        "  test_dl = DataLoader(test_ds, batch_size=batch_size, num_workers=1, \n",
        "                        shuffle=True, drop_last=False,\n",
        "                        collate_fn=Collate(train_config['nx1']))\n",
        "  val_dl = DataLoader(val_ds, batch_size=batch_size, num_workers=1, \n",
        "                      shuffle=True, drop_last=False,\n",
        "                        collate_fn=Collate(train_config['nx1']))\n",
        "  pprint(trial.params)\n",
        "  pbar_epoch = tqdm(range(n_epoches))\n",
        "  for i in tqdm(pbar_epoch):\n",
        "    # train\n",
        "    train_scores = run('train', device, model, opt, loss_fn, train_dl, ds, \n",
        "                       print_triples=True, \n",
        "                       plot_attn=False if model_config['attn_type'] != 'none' else False)\n",
        "    loss, acc, pre, rec, f1 = train_scores\n",
        "    # validation\n",
        "    val_scores = run('val', device, model, opt, loss_fn, val_dl, ds, \n",
        "                     print_triples=True, \n",
        "                     plot_attn=False if model_config['attn_type'] != 'none' else False)\n",
        "    val_loss, val_acc, val_pre, val_rec, val_f1 = val_scores\n",
        "    pbar_epoch.set_description(f'l: {loss:.2f}/{val_loss:.2f} a: {int(acc*100)}/{int(val_acc*100)} p: {int(pre*100)}/{int(val_pre*100)} r: {int(rec*100)}/{int(val_rec*100)} f1: {int(f1*100)}/{int(val_f1*100)}')\n",
        "    trial.report(loss, step=i+1)\n",
        "    torch.save({'epoch': i,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "                'loss': loss,\n",
        "                'model_config': model_config,\n",
        "                'train_config': train_config},\n",
        "                fn)\n",
        "    try:\n",
        "      copyfile(fn, '/content/drive/My Drive/' + fn)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "  # test\n",
        "  test_scores = run('test', device, model, opt, loss_fn, test_dl, ds,\n",
        "                                                         print_triples=True, \n",
        "                                                         plot_attn=True if model_config['attn_type'] != 'none' else False)\n",
        "  test_loss, test_acc, test_pre, test_rec, test_f1 = test_scores\n",
        "  print(f'           train\\tval\\ttest')\n",
        "  print(f'     loss: {loss:.3f}\\t{val_loss:.3f}\\t{test_loss:.3f}')\n",
        "  print(f' accuracy: {acc:.3f}\\t{val_acc:.3f}\\t{test_acc:.3f}')\n",
        "  print(f'precision: {pre:.3f}\\t{val_pre:.3f}\\t{test_pre:.3f}')\n",
        "  print(f'   recall: {rec:.3f}\\t{val_rec:.3f}\\t{test_rec:.3f}')\n",
        "  print(f'       f1: {f1:.3f}\\t{val_f1:.3f}\\t{test_f1:.3f}')\n",
        "  trial.set_user_attr('train_loss', loss)\n",
        "  trial.set_user_attr('train_acc', acc)\n",
        "  trial.set_user_attr('train_pre', pre)\n",
        "  trial.set_user_attr('train_rec', rec)\n",
        "  trial.set_user_attr('train_f1', f1)\n",
        "  trial.set_user_attr('val_loss', val_loss)\n",
        "  trial.set_user_attr('val_acc', val_acc)\n",
        "  trial.set_user_attr('val_pre', val_pre)\n",
        "  trial.set_user_attr('val_rec', val_rec)\n",
        "  trial.set_user_attr('val_f1', val_f1)\n",
        "  trial.set_user_attr('test_loss', test_loss)\n",
        "  trial.set_user_attr('test_acc', test_acc)\n",
        "  trial.set_user_attr('test_pre', test_pre)\n",
        "  trial.set_user_attr('test_rec', test_rec)\n",
        "  trial.set_user_attr('test_f1', test_f1)\n",
        "  return loss\n",
        "\n",
        "fn = 'optuna.db'\n",
        "study = optuna.create_study(study_name='1', \n",
        "                            direction='minimize', \n",
        "                            storage=f'sqlite:///{fn}', \n",
        "                            load_if_exists=True)\n",
        "study.optimize(objective, n_trials=10)\n",
        "try:\n",
        "  copyfile(fn, '/content/drive/My Drive/' + fn)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlcXPvqiyHbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try:\n",
        "    \n",
        "# except KeyboardInterrupt:\n",
        "#   print(e)\n",
        "# except Exception as e:\n",
        "#   print(e)\n",
        "# finally:\n",
        "#   torch.save({'epoch': i,\n",
        "#               'model_state_dict': model.state_dict(),\n",
        "#               'optimizer_state_dict': opt.state_dict(),\n",
        "#               'loss': loss,\n",
        "#               'model_config': model_config,\n",
        "#               'train_config': train_config},\n",
        "#               'model_exception.pth')\n",
        "#   try:\n",
        "#     copyfile('model.pth', '/content/drive/My Drive/' + 'model.pth')\n",
        "#   except Exception as e:\n",
        "#     print(e)\n",
        "# raise KeyboardInterrupt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJernh1FIvU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(study.best_params)\n",
        "pprint(study.best_value)\n",
        "pprint(study.best_trial)\n",
        "pprint(study.direction)\n",
        "%load_ext google.colab.data_table\n",
        "study.trials_dataframe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgmudFU2IykS",
        "colab_type": "code",
        "outputId": "f3d6af66-e6f1-4701-a681-c52eac8a81fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "# optuna.visualization.plot_contour(study, params=['out_hidden', 'attn_type'])\n",
        "# optuna.visualization.plot_optimization_history(study)\n",
        "# optuna.visualization.plot_slice(study)\n",
        "# optuna.visualization.plot_parallel_coordinate(study)  # BUG\n",
        "optuna.visualization.plot_intermediate_values(study)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b77b9220-5b07-4085-af80-dc89f34d6e3e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b77b9220-5b07-4085-af80-dc89f34d6e3e\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b77b9220-5b07-4085-af80-dc89f34d6e3e',\n",
              "                        [{\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial2\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [3.74948787689209, 3.7344651222229004, 3.4250125885009766, 3.0013296604156494, 2.4971694946289062, 3.4377706050872803, 2.6455793380737305, 1.7869172096252441, 2.0913965702056885, 1.6921584606170654]}],\n",
              "                        {\"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Intermediate Values Plot\"}, \"xaxis\": {\"title\": {\"text\": \"Step\"}}, \"yaxis\": {\"title\": {\"text\": \"Intermediate Value\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b77b9220-5b07-4085-af80-dc89f34d6e3e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PmMVbh1u7Hs",
        "colab_type": "code",
        "outputId": "03c6f283-c5b3-4fa2-9e0d-ba7598fed309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "checkpoint = torch.load('model.pth')\n",
        "\n",
        "model = EncoderDecoder(**checkpoint['model_config']).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljDhsQTRtw3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "def inference(model, ds, s1):\n",
        "  ex2 = [ds.stoi['<go>']]\n",
        "  ex1 = ds.encode(s1)\n",
        "  # print(ex1)\n",
        "  # print(ex2)\n",
        "  # print(ds.decode(ex1))\n",
        "  # print(ds.decode(ex2))\n",
        "  max_len = 10\n",
        "  model.eval()\n",
        "  cnt = 1\n",
        "  with torch.no_grad():\n",
        "    while ex2[-1] != ds.stoi['<eos>'] and cnt < max_len:\n",
        "      x1 = torch.tensor([ex1]).to(device)\n",
        "      x2 = torch.tensor([ex2]).to(device)\n",
        "      y2, attn = model(x1, x2)\n",
        "      # print(y2.size())\n",
        "      p = y2[:,-1,:].flatten(0, 1)\n",
        "      # print(p)\n",
        "      px2 = torch.argmax(p)\n",
        "      # print(p2.item())\n",
        "      ex2.append(px2.item())\n",
        "      cnt += 1\n",
        "  return ds.decode(ex2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOVHE5uvu-pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "title = 'Агент Жиго отреагировал на новость об интересе «Лиона» к футболисту «Спартака»'\n",
        "text = 'Мохамед Беншенафи, агент защитника московского «Спартака» Самуэля Жиго, опроверг информацию об интересе к игроку со стороны французского футбольного клуба «Лион».'\n",
        "ss = [' '.join([w.lower() for w in word_tokenize(s) if w.isalnum()]) for s in sent_tokenize(text)]\n",
        "pred_title = inference(model, ds, ss[0])\n",
        "print(ss)\n",
        "print(title)\n",
        "print(pred_title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D66ozYfGBrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "import gensim.downloader as api\n",
        "info = api.info()\n",
        "pprint(info['models'].keys())\n",
        "vec_model = api.load('word2vec-ruscorpora-300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2jxTLolSk82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/akutuzov/webvectors/blob/master/preprocessing/rusvectores_tutorial.ipynb\n",
        "\n",
        "# print(model.most_similar(\"кто\"))\n",
        "print(len(model.vocab))\n",
        "print(list(model.vocab)[:100])\n",
        "print(dir(model))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok6D-od_TksD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqQjM-JMjBZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ufal.udpipe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVf_m2FRkt2A",
        "colab_type": "text"
      },
      "source": [
        "https://universaldependencies.org/u/pos/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D-vRBwLVw1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "# import wget\n",
        "import re\n",
        "from ufal.udpipe import Model, Pipeline\n",
        "\n",
        "'''\n",
        "Этот скрипт принимает на вход необработанный русский текст \n",
        "(одно предложение на строку или один абзац на строку).\n",
        "Он токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
        "На выход подаётся последовательность разделенных пробелами лемм с частями речи \n",
        "(\"зеленый_NOUN трамвай_NOUN\").\n",
        "Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).\n",
        "Примеры запуска:\n",
        "echo 'Мама мыла раму.' | python3 rus_preprocessing_udpipe.py\n",
        "zcat large_corpus.txt.gz | python3 rus_preprocessing_udpipe.py | gzip > processed_corpus.txt.gz\n",
        "'''\n",
        "\n",
        "\n",
        "def num_replace(word):\n",
        "    newtoken = 'x' * len(word)\n",
        "    return newtoken\n",
        "\n",
        "\n",
        "def clean_token(token, misc):\n",
        "    \"\"\"\n",
        "    :param token:  токен (строка)\n",
        "    :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
        "    :return: очищенный токен (строка)\n",
        "    \"\"\"\n",
        "    out_token = token.strip().replace(' ', '')\n",
        "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
        "        return None\n",
        "    return out_token\n",
        "\n",
        "\n",
        "def clean_lemma(lemma, pos):\n",
        "    \"\"\"\n",
        "    :param lemma: лемма (строка)\n",
        "    :param pos: часть речи (строка)\n",
        "    :return: очищенная лемма (строка)\n",
        "    \"\"\"\n",
        "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
        "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
        "        return None\n",
        "    if pos != 'PUNCT':\n",
        "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
        "            out_lemma = ''.join(out_lemma[1:])\n",
        "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
        "            out_lemma = ''.join(out_lemma[:-1])\n",
        "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
        "                or out_lemma.endswith('.'):\n",
        "            out_lemma = ''.join(out_lemma[:-1])\n",
        "    return out_lemma\n",
        "\n",
        "\n",
        "def list_replace(search, replacement, text):\n",
        "    search = [el for el in search if el in text]\n",
        "    for c in search:\n",
        "        text = text.replace(c, replacement)\n",
        "    return text\n",
        "\n",
        "\n",
        "def unify_sym(text):  # принимает строку в юникоде\n",
        "    text = list_replace \\\n",
        "        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
        "\n",
        "    text = list_replace \\\n",
        "        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
        "\n",
        "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
        "\n",
        "    text = list_replace \\\n",
        "            (\n",
        "            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
        "            '\\u2002', text)\n",
        "\n",
        "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
        "    text = re.sub('\\t\\t', '\\t', text)\n",
        "\n",
        "    text = list_replace \\\n",
        "            (\n",
        "            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
        "            '.', text)\n",
        "\n",
        "    text = list_replace('\\u2217', '\\u002A', text)\n",
        "\n",
        "    text = list_replace('…', '...', text)\n",
        "\n",
        "    text = list_replace('\\u2241\\u224B\\u2E2F\\u0483', '\\u223D', text)\n",
        "\n",
        "    text = list_replace('\\u00C4', 'A', text)  # латинская\n",
        "    text = list_replace('\\u00E4', 'a', text)\n",
        "    text = list_replace('\\u00CB', 'E', text)\n",
        "    text = list_replace('\\u00EB', 'e', text)\n",
        "    text = list_replace('\\u1E26', 'H', text)\n",
        "    text = list_replace('\\u1E27', 'h', text)\n",
        "    text = list_replace('\\u00CF', 'I', text)\n",
        "    text = list_replace('\\u00EF', 'i', text)\n",
        "    text = list_replace('\\u00D6', 'O', text)\n",
        "    text = list_replace('\\u00F6', 'o', text)\n",
        "    text = list_replace('\\u00DC', 'U', text)\n",
        "    text = list_replace('\\u00FC', 'u', text)\n",
        "    text = list_replace('\\u0178', 'Y', text)\n",
        "    text = list_replace('\\u00FF', 'y', text)\n",
        "    text = list_replace('\\u00DF', 's', text)\n",
        "    text = list_replace('\\u1E9E', 'S', text)\n",
        "\n",
        "    currencies = list \\\n",
        "            (\n",
        "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
        "        )\n",
        "\n",
        "    alphabet = list \\\n",
        "            (\n",
        "            '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!?~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
        "\n",
        "    alphabet.append(\"'\")\n",
        "\n",
        "    allowed = set(currencies + alphabet)\n",
        "\n",
        "    cleaned_text = [sym for sym in text if sym in allowed]\n",
        "    cleaned_text = ''.join(cleaned_text)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
        "    # Если частеречные тэги не нужны (например, их нет в модели), выставьте pos=False\n",
        "    # в этом случае на выход будут поданы только леммы\n",
        "    # По умолчанию знаки пунктуации вырезаются. Чтобы сохранить их, выставьте punct=True\n",
        "\n",
        "    entities = {'PROPN'}\n",
        "    named = False\n",
        "    memory = []\n",
        "    mem_case = None\n",
        "    mem_number = None\n",
        "    tagged_propn = []\n",
        "\n",
        "    # обрабатываем текст, получаем результат в формате conllu:\n",
        "    processed = pipeline.process(text)\n",
        "\n",
        "    # пропускаем строки со служебной информацией:\n",
        "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
        "\n",
        "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
        "    tagged = [w.split('\\t') for w in content if w]\n",
        "\n",
        "    for t in tagged:\n",
        "        if len(t) != 10:\n",
        "            continue\n",
        "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
        "        token = clean_token(token, misc)\n",
        "        lemma = clean_lemma(lemma, pos)\n",
        "        if not lemma or not token:\n",
        "            continue\n",
        "        if pos in entities:\n",
        "            if '|' not in feats:\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "                continue\n",
        "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
        "            if 'Case' not in morph or 'Number' not in morph:\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "                continue\n",
        "            if not named:\n",
        "                named = True\n",
        "                mem_case = morph['Case']\n",
        "                mem_number = morph['Number']\n",
        "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
        "                memory.append(lemma)\n",
        "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
        "                    named = False\n",
        "                    past_lemma = '::'.join(memory)\n",
        "                    memory = []\n",
        "                    tagged_propn.append(past_lemma + '_PROPN')\n",
        "            else:\n",
        "                named = False\n",
        "                past_lemma = '::'.join(memory)\n",
        "                memory = []\n",
        "                tagged_propn.append(past_lemma + '_PROPN')\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "        else:\n",
        "            if not named:\n",
        "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
        "                    lemma = num_replace(token)\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "            else:\n",
        "                named = False\n",
        "                past_lemma = '::'.join(memory)\n",
        "                memory = []\n",
        "                tagged_propn.append(past_lemma + '_PROPN')\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "\n",
        "    if not keep_punct:\n",
        "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
        "    if not keep_pos:\n",
        "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
        "    return tagged_propn\n",
        "\n",
        "import requests\n",
        "import os \n",
        "\n",
        "# URL of the UDPipe model\n",
        "udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
        "udpipe_filename = udpipe_model_url.split('/')[-1]\n",
        "\n",
        "if not os.path.isfile(udpipe_filename):\n",
        "    print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
        "    r = requests.get(udpipe_model_url)\n",
        "    with open(udpipe_filename, 'wb') as f:\n",
        "      f.write(r.content)\n",
        "\n",
        "print('\\nLoading the model...', file=sys.stderr)\n",
        "model = Model.load(udpipe_filename)\n",
        "process_pipeline = Pipeline(model, 'tokenize', \n",
        "                            Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
        "\n",
        "# print('Processing input...', file=sys.stderr)\n",
        "# for line in sys.stdin:\n",
        "#     res = unify_sym(line.strip())\n",
        "#     output = process(process_pipeline, text=res)\n",
        "#     print(' '.join(output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1Z5UPINJn0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torch.utils.data import Dataset, IterableDataset, DataLoader, random_split\n",
        "# import json\n",
        "\n",
        "# from tqdm.notebook import tqdm\n",
        "\n",
        "# news = []\n",
        "# n_news = 1000\n",
        "# with gzip.open(comp_fn, 'rb') as gz_file:\n",
        "#   for _ in tqdm(range(n_news)):\n",
        "#     news.append(json.loads(next(gz_file)))\n",
        "# print(news[0])\n",
        "# print(len(news))\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# words = Counter()\n",
        "# for n in news:\n",
        "#   words.update(w for s in n['text'] for w in s.split())\n",
        "#   words.update(w for s in n['title'] for w in s.split())\n",
        "# print(words.most_common(10))\n",
        "# print(len(words))\n",
        "\n",
        "# from torchtext.vocab import Vocab, SubwordVocab\n",
        "# min_freq = 0\n",
        "# max_size = 40000\n",
        "# voc = Vocab(counter=words, max_size=max_size, specials=['<pad>', '<unk>', '<go>', '<eos>'])\n",
        "# # voc = SubwordVocab(counter=words, max_size=max_size, specials=['<pad>', '<unk>', '<go>', '<eos>'])\n",
        "# print(len(voc))\n",
        "# print(dir(voc))\n",
        "# print(voc.stoi)\n",
        "\n",
        "# class RiaDataset(IterableDataset):\n",
        "#   def __init__(self, path, start, end, vocab):\n",
        "#     super(RiaDataset).__init__()\n",
        "#     self.path = path\n",
        "#     self.start = start\n",
        "#     self.end = end\n",
        "#     self.vocab = vocab\n",
        "\n",
        "#   def __len__(self):\n",
        "#     return self.end - self.start\n",
        "\n",
        "#   def __iter__(self):\n",
        "#     worker_info = torch.utils.data.get_worker_info()\n",
        "#     # print(worker_info.id, id(self), self.start, self.end)\n",
        "#     with open(self.path) as f:\n",
        "#       for _ in range(self.start):\n",
        "#         next(f)\n",
        "#       for _ in range(self.start, self.end):\n",
        "#         yield self.preprocess(next(f))\n",
        "\n",
        "#   def preprocess(self, data):\n",
        "#     data = json.loads(data)\n",
        "#     enc_text = [[self.vocab.stoi.get(w, 1) for w in s.split()] for s in data['text']]\n",
        "#     enc_title = [[self.vocab.stoi.get(w, 1) for w in s.split()] for s in data['title']]\n",
        "#     # dec_text = [[self.vocab.itos[w] for w in s] for s in enc_text]\n",
        "#     # enc_text = [[w for w in s.split()] for s in data['text']]\n",
        "#     # enc_text = [w for s in data['text'] for w in s.split()]\n",
        "#     # enc_title = [w for s in data['title'] for w in s.split()]\n",
        "#     enc_title = [self.vocab.stoi.get(w, 1) for s in data['title'] for w in s.split()]\n",
        "#     enc_text = [self.vocab.stoi.get(w, 1) for s in data['text'] for w in s.split()][:len(enc_title)*3]\n",
        "#     return [enc_text, enc_title]\n",
        "\n",
        "# def worker_init_fn(worker_id):\n",
        "#   worker_info = torch.utils.data.get_worker_info()\n",
        "#   s, e = worker_info.dataset.start, worker_info.dataset.end\n",
        "#   n = worker_info.num_workers\n",
        "#   q, r = (e - s) // n, (e - s) % n\n",
        "#   s = s + worker_id * q\n",
        "#   e = s + q if worker_id < n - 1 else s + q + r\n",
        "#   worker_info.dataset.start, worker_info.dataset.end = s, e\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#   bx1, bx2 = [], []\n",
        "#   for b in batch:\n",
        "#     bx1.append(torch.tensor(b[0]))\n",
        "#     bx2.append(torch.tensor(b[1]))\n",
        "#   bx1 = torch.nn.utils.rnn.pad_sequence(bx1, batch_first=True)\n",
        "#   bx2 = torch.nn.utils.rnn.pad_sequence(bx2, batch_first=True)\n",
        "#   batch = [bx1, bx2]\n",
        "#   return batch\n",
        "\n",
        "# ds = RiaDataset('norm_sents_ria_1k.json', 0, 100, voc)\n",
        "# dl = DataLoader(ds, batch_size=3, num_workers=1, shuffle=False, drop_last=False, \n",
        "#                                  collate_fn=collate_fn, worker_init_fn=worker_init_fn)\n",
        "# for b in tqdm(dl):\n",
        "#   print(b[0].size(), b[1].size())\n",
        "#   print(len(b))\n",
        "# # train_len = int(0.7*len(ds))\n",
        "# # test_len = int(0.2*len(ds))\n",
        "# # val_len = len(ds) - train_len - test_len\n",
        "# # lens = [train_len, test_len, val_len]\n",
        "# # print(lens)\n",
        "# # train_ds, test_ds, val_ds = random_split(ds, lens)\n",
        "# # dl = DataLoader(train_ds, batch_size=2, num_workers=1, shuffle=False, drop_last=False, \n",
        "# #                                  collate_fn=collate_fn, worker_init_fn=worker_init_fn)\n",
        "# # # print(dir(train_ds))\n",
        "# # # print(train_ds.indices)\n",
        "# # for b in tqdm(dl):\n",
        "# #   print(b[0].size(), b[1].size())\n",
        "# #   print(len(b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEHx2moy6Ckg",
        "colab_type": "text"
      },
      "source": [
        "http://wiki.nlpl.eu/index.php/Home"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdXjzRxQkdfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "model_url = 'http://vectors.nlpl.eu/repository/20/184.zip'  # Russian News\n",
        "fn = os.path.basename(model_url)\n",
        "if not os.path.exists(fn):\n",
        "  r = requests.get(model_url)\n",
        "  with open(fn, 'wb') as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "with zipfile.ZipFile(fn) as f:\n",
        "  stream = f.open('model.bin')\n",
        "  vec_model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88lUEGlppGo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_model = api.load('word2vec-ruscorpora-300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxZ4Ua-bh2k6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = news[0]['text'][1]\n",
        "print(text)\n",
        "tagged = process(process_pipeline, text)\n",
        "print(tagged)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SColCrJtpRJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_model = api.load('word2vec-ruscorpora-300')\n",
        "vs = [vec_model[x].shape for x in tagged if x in vec_model]\n",
        "print(vec_model[tagged[0]])\n",
        "print(len(tagged), len(vs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-mIgv8OyBZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "print(multiprocessing.cpu_count())\n",
        "print(len(os.sched_getaffinity(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gnoOA0InXYE",
        "colab_type": "text"
      },
      "source": [
        "http://vectors.nlpl.eu/repository/"
      ]
    }
  ]
}